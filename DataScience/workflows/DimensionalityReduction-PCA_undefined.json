{	"name": "Dimensionality Reduction - PCA",	"uuid": "30c26f42-a079-429e-9911-8b52ffeb2627",	"category": "Dimensionality Reduction",	"description": "Housing Price Prediction",	"parameters": "-",	"nodes": [		{			"id": "2",			"name": "VectorAssembler",			"description": "Merges multiple columns into a vector column",			"details": "VectorAssembler is a transformer that combines a given list of columns into a single vector column. <br>\nIt is useful for combining raw features and features generated by different feature transformers into a single feature vector, in order to train ML models like logistic regression and decision trees. <br>\nVectorAssembler accepts the following input column types: all numeric types, boolean type, and vector type. In each row, the values of the input columns will be concatenated into a vector in the specified order.<br>\n<br>\nMore details are available at:<br>\n<br>\n<a href=\"https://spark.apache.org/docs/latest/ml-features.html#vectorassembler\" target=\"_blank\">spark.apache.org/docs/latest/ml-features.html#vectorassembler</a><br>",			"examples": "<h2>The below example is available at : <a href=\"https://spark.apache.org/docs/latest/ml-features.html#vectorassembler\" target=\"_blank\">spark.apache.org/docs/latest/ml-features.html#vectorassembler</a></h2>\n<br>\nimport org.apache.spark.ml.feature.VectorAssembler<br>\nimport org.apache.spark.ml.linalg.Vectors<br>\n<br>\nval dataset = spark.createDataFrame(<br>\n  Seq((0, 18, 1.0, Vectors.dense(0.0, 10.0, 0.5), 1.0))<br>\n).toDF(\"id\", \"hour\", \"mobile\", \"userFeatures\", \"clicked\")<br>\n<br>\nval assembler = new VectorAssembler()<br>\n  .setInputCols(Array(\"hour\", \"mobile\", \"userFeatures\"))<br>\n  .setOutputCol(\"features\")<br>\n<br>\nval output = assembler.transform(dataset)<br>\nprintln(\"Assembled columns 'hour', 'mobile', 'userFeatures' to vector column 'features'\")<br>\noutput.select(\"features\", \"clicked\").show(false)<br>",			"type": "ml-transformer",			"nodeClass": "fire.nodes.ml.NodeVectorAssembler",			"x": "372.719px",			"y": "147.719px",			"fields": [				{					"name": "storageLevel",					"value": "DEFAULT",					"widget": "array",					"title": "Output Storage Level",					"description": "Storage Level of the Output Datasets of this Node",					"optionsArray": [						"DEFAULT",						"NONE",						"DISK_ONLY",						"DISK_ONLY_2",						"MEMORY_ONLY",						"MEMORY_ONLY_2",						"MEMORY_ONLY_SER",						"MEMORY_ONLY_SER_2",						"MEMORY_AND_DISK",						"MEMORY_AND_DISK_2",						"MEMORY_AND_DISK_SER",						"MEMORY_AND_DISK_SER_2",						"OFF_HEAP"					],					"required": false,					"display": true,					"editable": true,					"disableRefresh": false				},				{					"name": "inputCols",					"value": "[\"lotsize\",\"stories\",\"garagepl\",\"driveway_idx\",\"recroom_idx\",\"fullbase_idx\",\"bashw_idx\",\"airco_idx\",\"prefarea_idx\",\"bedrooms_oh\",\"bathrms_oh\"]",					"widget": "variables",					"title": "Input Columns",					"description": "Input column of type - all numeric, boolean and vector",					"datatypes": [						"integer",						"long",						"double",						"float",						"vectorudt"					],					"required": false,					"display": true,					"editable": true,					"disableRefresh": false				},				{					"name": "outputCol",					"value": "features",					"widget": "textfield",					"title": "Output Column",					"description": "Output column name",					"required": true,					"display": true,					"editable": true,					"disableRefresh": false				},				{					"name": "handleInvalid",					"value": "error",					"widget": "array",					"title": "HandleInvalid",					"description": "How to handle invalid data (NULL values). Options are 'skip' (filter out rows with invalid data), 'error' (throw an error), or 'keep' (return relevant number of NaN in the output).",					"optionsArray": [						"error",						"skip",						"keep"					],					"required": false,					"display": true,					"editable": true,					"disableRefresh": false				}			],			"engine": "scala"		},		{			"id": "12",			"name": "StringIndexer",			"description": "StringIndexer encodes a string column of labels to a column of label indices",			"details": "StringIndexer encodes a string column of labels to a column of label indices. The indices are in [0, numLabels), ordered by label frequencies, so the most frequent label gets index 0.<br>\nIf the input column is numeric, we cast it to string and index the string values.<br>\n                                                                                                  <br>\nMore at Spark MLlib/ML docs page : <a href=\"https://spark.apache.org/docs/2.0.0/ml-features.html#stringindexer\" target=\"_blank\">spark.apache.org/docs/2.0.0/ml-features.html#stringindexer</a><br>",			"examples": "<h2>The below example is available at : <a href=\"https://spark.apache.org/docs/2.0.0/ml-features.html#stringindexer\" target=\"_blank\">spark.apache.org/docs/2.0.0/ml-features.html#stringindexer</a></h2>\n<br>\nimport org.apache.spark.ml.feature.StringIndexer<br>\n<br>\nval df = spark.createDataFrame(<br>\n  Seq((0, \"a\"), (1, \"b\"), (2, \"c\"), (3, \"a\"), (4, \"a\"), (5, \"c\"))<br>\n).toDF(\"id\", \"category\")<br>\n<br>\nval indexer = new StringIndexer()<br>\n  .setInputCol(\"category\")<br>\n  .setOutputCol(\"categoryIndex\")<br>\n<br>\nval indexed = indexer.fit(df).transform(df)<br>\nindexed.show()<br>",			"type": "ml-transformer",			"nodeClass": "fire.nodes.ml.NodeStringIndexer",			"x": "387.594px",			"y": "513.719px",			"fields": [				{					"name": "storageLevel",					"value": "DEFAULT",					"widget": "array",					"title": "Output Storage Level",					"description": "Storage Level of the Output Datasets of this Node",					"optionsArray": [						"DEFAULT",						"NONE",						"DISK_ONLY",						"DISK_ONLY_2",						"MEMORY_ONLY",						"MEMORY_ONLY_2",						"MEMORY_ONLY_SER",						"MEMORY_ONLY_SER_2",						"MEMORY_AND_DISK",						"MEMORY_AND_DISK_2",						"MEMORY_AND_DISK_SER",						"MEMORY_AND_DISK_SER_2",						"OFF_HEAP"					],					"required": false,					"display": true,					"editable": true,					"disableRefresh": false				},				{					"name": "handleInvalid",					"value": "error",					"widget": "array",					"title": "Handle Invalid",					"description": "Invalid entries to be skipped or thrown error",					"optionsArray": [						"skip",						"error"					],					"required": false,					"display": true,					"editable": true,					"disableRefresh": false				},				{					"name": "inputCols",					"value": "[\"driveway\",\"recroom\",\"fullbase\",\"gashw\",\"airco\",\"prefarea\"]",					"widget": "variables_list_select",					"title": "Input Columns",					"description": "Input columns for encoding",					"datatypes": [						"string"					],					"required": false,					"display": true,					"editable": true,					"disableRefresh": false				},				{					"name": "outputCols",					"value": "[\"driveway_idx\",\"recroom_idx\",\"fullbase_idx\",\"bashw_idx\",\"airco_idx\",\"prefarea_idx\"]",					"widget": "variables_list_textfield",					"title": "Output Columns",					"description": "Output columns",					"required": false,					"display": true,					"editable": true,					"disableRefresh": false				}			],			"engine": "scala"		},		{			"id": "13",			"name": "FlagOutlier",			"description": "Flag the outlier based on the selected column using Box-and-Whisker technique.",			"details": "This node identifies whether a data point is outlier in a series or not based on the lower and upper quantile set. <br>\n<br>\nIt can be checked against numeric columns only.<br>",			"examples": "FlagOutlier node can be configured as below:<br>\n<br>\nINPUT COLUMN TO FLAG THE OUTLIER\t:\tSALARY<br>\nLOWERQUANTILE\t\t\t\t\t\t          :\t0.25<br>\nUPPERQUANTILE\t\t\t\t\t\t          :\t0.75<br>\n<br>\nNew column [is_SALARY_outlier] would be added to the output specifing whether a data point is outlier or not.<br>",			"type": "transform",			"nodeClass": "fire.nodes.ml.NodeFlagOutlier",			"x": "87.4375px",			"y": "322.438px",			"fields": [				{					"name": "storageLevel",					"value": "DEFAULT",					"widget": "array",					"title": "Output Storage Level",					"description": "Storage Level of the Output Datasets of this Node",					"optionsArray": [						"DEFAULT",						"NONE",						"DISK_ONLY",						"DISK_ONLY_2",						"MEMORY_ONLY",						"MEMORY_ONLY_2",						"MEMORY_ONLY_SER",						"MEMORY_ONLY_SER_2",						"MEMORY_AND_DISK",						"MEMORY_AND_DISK_2",						"MEMORY_AND_DISK_SER",						"MEMORY_AND_DISK_SER_2",						"OFF_HEAP"					],					"required": false,					"display": true,					"editable": true,					"disableRefresh": false				},				{					"name": "inputCol",					"value": "price",					"widget": "variable",					"title": "Input Column to flag the outlier",					"description": "The Input Column to flag the outlier",					"datatypes": [						"double",						"integer",						"float"					],					"required": true,					"display": true,					"editable": true,					"disableRefresh": false				},				{					"name": "lowerQuantile",					"value": "0.25",					"widget": "textfield",					"title": "LowerQuantile",					"description": "",					"required": true,					"display": true,					"editable": true,					"disableRefresh": false				},				{					"name": "upperQuantile",					"value": "0.75",					"widget": "textfield",					"title": "UpperQuantile",					"description": "",					"required": true,					"display": true,					"editable": true,					"disableRefresh": false				}			],			"engine": "scala"		},		{			"id": "14",			"name": "RowFilter",			"description": "This node creates a new DataFrame containing the rows that satisfy the given condition",			"details": "This node creates a new DataFrame containing only rows satisfying the given condition.<br>",			"examples": "<h2>Examples of Conditional Expression</h2>\n<br>\nBelow are some examples of the Conditions Expression which can be used.<br>\n<br>\n<ul>\n<li> col1 > 5 AND col2 > 3</li>\n</ul>\n<ul>\n<li> name is not NULL</li>\n</ul>\n<ul>\n<li> name is NULL</li>\n</ul>\n<ul>\n<li> usd_pledged_real > 0 and (category = \"Narrative Film\" or category == \"Music\") and goal > 100</li>\n</ul>\n<ul>\n<li> dt > '2021-09-03'  (dt column is of type date)</li>\n</ul>\n<ul>\n<li> datetime > '2011-01-01 00:00:00.0'     (datetime column is of type timestamp)</li>\n</ul>\n<ul>\n<li> datetime > '2011-01-01 00:00:00.0' and datetime < '2016-01-01 00:00:00.0'</li>\n</ul>",			"type": "transform",			"nodeClass": "fire.nodes.etl.NodeRowFilter",			"x": "88.2188px",			"y": "513.672px",			"fields": [				{					"name": "storageLevel",					"value": "DEFAULT",					"widget": "array",					"title": "Output Storage Level",					"description": "Storage Level of the Output Datasets of this Node",					"optionsArray": [						"DEFAULT",						"NONE",						"DISK_ONLY",						"DISK_ONLY_2",						"MEMORY_ONLY",						"MEMORY_ONLY_2",						"MEMORY_ONLY_SER",						"MEMORY_ONLY_SER_2",						"MEMORY_AND_DISK",						"MEMORY_AND_DISK_2",						"MEMORY_AND_DISK_SER",						"MEMORY_AND_DISK_SER_2",						"OFF_HEAP"					],					"required": false,					"display": true,					"editable": true,					"disableRefresh": false				},				{					"name": "conditionExpr",					"value": "is_price_outlier == \"NO\"",					"widget": "textarea_small",					"title": "Conditional Expression",					"description": "The filtering condition. Rows not satisfying given condition will be excluded from output DataFrame. eg: usd_pledged_real > 0 and (category = 1 or category == 2) and goal > 100",					"required": true,					"display": true,					"editable": true,					"disableRefresh": false				}			],			"engine": "all"		},		{			"id": "15",			"name": "ReadCSV",			"description": "It reads in CSV files and creates a DataFrame from it",			"details": "This node reads CSV files and creates a DataFrame from it.<br>",			"examples": "",			"type": "dataset",			"nodeClass": "fire.nodes.dataset.NodeDatasetCSV",			"x": "82px",			"y": "141px",			"hint": "Whenever the file is changed, Refresh the Schema",			"fields": [				{					"name": "storageLevel",					"value": "DEFAULT",					"widget": "array",					"title": "Output Storage Level",					"description": "Storage Level of the Output Datasets of this Node",					"optionsArray": [						"DEFAULT",						"NONE",						"DISK_ONLY",						"DISK_ONLY_2",						"MEMORY_ONLY",						"MEMORY_ONLY_2",						"MEMORY_ONLY_SER",						"MEMORY_ONLY_SER_2",						"MEMORY_AND_DISK",						"MEMORY_AND_DISK_2",						"MEMORY_AND_DISK_SER",						"MEMORY_AND_DISK_SER_2",						"OFF_HEAP"					],					"required": false,					"display": true,					"editable": true,					"disableRefresh": false				},				{					"name": "path",					"value": "data/housing.csv",					"widget": "textfield",					"title": "Path",					"description": "Path of the Text file/directory",					"required": true,					"display": true,					"editable": true,					"disableRefresh": false				},				{					"name": "separator",					"value": ",",					"widget": "textfield",					"title": "Separator",					"description": "CSV Separator",					"required": false,					"display": true,					"editable": true,					"disableRefresh": false				},				{					"name": "header",					"value": "true",					"widget": "array",					"title": "Header",					"description": "Does the file have a header row",					"optionsArray": [						"true",						"false"					],					"required": false,					"display": true,					"editable": true,					"disableRefresh": false				},				{					"name": "dropSpecialCharacterInColumnName",					"value": "true",					"widget": "array",					"title": "Drop Special Character In ColumnName",					"description": "Drop the SpecialCharacter and Space in Column Name.",					"optionsArray": [						"true",						"false"					],					"required": false,					"display": true,					"editable": true,					"disableRefresh": false				},				{					"name": "dropMalformed",					"value": "false",					"widget": "array",					"title": "Drop Malformed",					"description": "Whether to drop Malformed records or error",					"optionsArray": [						"true",						"false"					],					"required": false,					"display": true,					"editable": true,					"disableRefresh": false				},				{					"name": "outputColNames",					"value": "[\"id\",\"price\",\"lotsize\",\"bedrooms\",\"bathrms\",\"stories\",\"driveway\",\"recroom\",\"fullbase\",\"gashw\",\"airco\",\"garagepl\",\"prefarea\"]",					"widget": "schema_col_names",					"title": "Column Names for the CSV",					"description": "New Output Columns of the SQL",					"required": false,					"display": true,					"editable": true,					"disableRefresh": false				},				{					"name": "outputColTypes",					"value": "[\"INTEGER\",\"DOUBLE\",\"INTEGER\",\"INTEGER\",\"INTEGER\",\"INTEGER\",\"STRING\",\"STRING\",\"STRING\",\"STRING\",\"STRING\",\"INTEGER\",\"STRING\"]",					"widget": "schema_col_types",					"title": "Column Types for the CSV",					"description": "Data Type of the Output Columns",					"required": false,					"display": true,					"editable": true,					"disableRefresh": false				},				{					"name": "outputColFormats",					"value": "[\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\"]",					"widget": "schema_col_formats",					"title": "Column Formats for the CSV",					"description": "Format of the Output Columns",					"required": false,					"display": true,					"editable": true,					"disableRefresh": false				}			],			"engine": "all"		},		{			"id": "18",			"name": "OneHotEncoder",			"description": "Maps a column of label indices to a column of binary vectors, with at most a single one-value",			"details": "One-hot encoding maps a column of label indices to a column of binary vectors, with at most a single one-value. <br>\nThis encoding allows algorithms which expect continuous features, such as Logistic Regression, to use categorical features.<br>\n<br>\nMore at Spark MLlib/ML docs page : <a href=\"https://spark.apache.org/docs/2.0.0/ml-features.html#onehotencoder\" target=\"_blank\">spark.apache.org/docs/2.0.0/ml-features.html#onehotencoder</a><br>",			"examples": "<h2>The below example is available at : <a href=\"https://spark.apache.org/docs/2.0.0/ml-features.html#onehotencoder\" target=\"_blank\">spark.apache.org/docs/2.0.0/ml-features.html#onehotencoder</a></h2>\n<br>\nimport org.apache.spark.ml.feature.{OneHotEncoder, StringIndexer}<br>\n<br>\nval df = spark.createDataFrame(Seq(<br>\n  (0, \"a\"),<br>\n  (1, \"b\"),<br>\n  (2, \"c\"),<br>\n  (3, \"a\"),<br>\n  (4, \"a\"),<br>\n  (5, \"c\")<br>\n)).toDF(\"id\", \"category\")<br>\n<br>\nval indexer = new StringIndexer()<br>\n  .setInputCol(\"category\")<br>\n  .setOutputCol(\"categoryIndex\")<br>\n  .fit(df)<br>\nval indexed = indexer.transform(df)<br>\n<br>\nval encoder = new OneHotEncoder()<br>\n  .setInputCol(\"categoryIndex\")<br>\n  .setOutputCol(\"categoryVec\")<br>\nval encoded = encoder.transform(indexed)<br>\nencoded.select(\"id\", \"categoryVec\").show()<br>",			"type": "ml-transformer",			"nodeClass": "fire.nodes.ml.NodeOneHotEncoder",			"x": "380px",			"y": "331px",			"fields": [				{					"name": "storageLevel",					"value": "DEFAULT",					"widget": "array",					"title": "Output Storage Level",					"description": "Storage Level of the Output Datasets of this Node",					"optionsArray": [						"DEFAULT",						"NONE",						"DISK_ONLY",						"DISK_ONLY_2",						"MEMORY_ONLY",						"MEMORY_ONLY_2",						"MEMORY_ONLY_SER",						"MEMORY_ONLY_SER_2",						"MEMORY_AND_DISK",						"MEMORY_AND_DISK_2",						"MEMORY_AND_DISK_SER",						"MEMORY_AND_DISK_SER_2",						"OFF_HEAP"					],					"required": false,					"display": true,					"editable": true,					"disableRefresh": false				},				{					"name": "inputCols",					"value": "[\"bedrooms\",\"bathrms\"]",					"widget": "variables_list_select",					"title": "Input Columns",					"description": "Input columns for encoding",					"datatypes": [						"integer",						"long",						"double",						"float"					],					"required": false,					"display": true,					"editable": true,					"disableRefresh": false				},				{					"name": "outputCols",					"value": "[\"bedrooms_oh\",\"bathrms_oh\"]",					"widget": "variables_list_textfield",					"title": "Output Columns",					"description": "Output columns",					"required": false,					"display": true,					"editable": true,					"disableRefresh": false				}			],			"engine": "scala"		},		{			"id": "19",			"name": "PCA",			"description": "Trains a model to project vectors to a low-dimensional space using PCA.",			"details": "Principal component analysis (PCA) is a statistical method to find a rotation such that the first coordinate has the largest variance possible, and each succeeding coordinate in turn has the largest variance possible. <br>\nThe columns of the rotation matrix are called principal components.<br>\n<br>\nMore at Spark MLlib/ML docs page : <a href=\"https://spark.apache.org/docs/2.0.0/mllib-dimensionality-reduction.html#principal-component-analysis-pca\" target=\"_blank\">spark.apache.org/docs/2.0.0/mllib-dimensionality-reduction.html#principal-component-analysis-pca</a><br>",			"examples": "<h2>The below example is available at : <a href=\"https://spark.apache.org/docs/2.0.0/mllib-dimensionality-reduction.html#principal-component-analysis-pca\" target=\"_blank\">spark.apache.org/docs/2.0.0/mllib-dimensionality-reduction.html#principal-component-analysis-pca</a></h2>\n<br>\nimport org.apache.spark.mllib.linalg.Matrix<br>\nimport org.apache.spark.mllib.linalg.Vectors<br>\nimport org.apache.spark.mllib.linalg.distributed.RowMatrix<br>\n<br>\nval data = Array(<br>\n  Vectors.sparse(5, Seq((1, 1.0), (3, 7.0))),<br>\n  Vectors.dense(2.0, 0.0, 3.0, 4.0, 5.0),<br>\n  Vectors.dense(4.0, 0.0, 0.0, 6.0, 7.0))<br>\n<br>\nval dataRDD = sc.parallelize(data, 2)<br>\n<br>\nval mat: RowMatrix = new RowMatrix(dataRDD)<br>\n<br>\n// Compute the top 4 principal components.<br>\n// Principal components are stored in a local dense matrix.<br>\nval pc: Matrix = mat.computePrincipalComponents(4)<br>\n<br>\n// Project the rows to the linear space spanned by the top 4 principal components.<br>\nval projected: RowMatrix = mat.multiply(pc)<br>",			"type": "ml-transformer",			"nodeClass": "fire.nodes.ml.NodePCA",			"x": "673px",			"y": "152px",			"fields": [				{					"name": "storageLevel",					"value": "DEFAULT",					"widget": "array",					"title": "Output Storage Level",					"description": "Storage Level of the Output Datasets of this Node",					"optionsArray": [						"DEFAULT",						"NONE",						"DISK_ONLY",						"DISK_ONLY_2",						"MEMORY_ONLY",						"MEMORY_ONLY_2",						"MEMORY_ONLY_SER",						"MEMORY_ONLY_SER_2",						"MEMORY_AND_DISK",						"MEMORY_AND_DISK_2",						"MEMORY_AND_DISK_SER",						"MEMORY_AND_DISK_SER_2",						"OFF_HEAP"					],					"required": false,					"display": true,					"editable": true,					"disableRefresh": false				},				{					"name": "inputCol",					"value": "features",					"widget": "variable",					"title": "Input Column",					"description": "The input column name",					"datatypes": [						"vectorudt"					],					"required": true,					"display": true,					"editable": true,					"disableRefresh": false				},				{					"name": "outputCol",					"value": "feature_pca",					"widget": "textfield",					"title": "Output Column",					"description": "The output column name",					"datatypes": [						"vectorudt"					],					"required": true,					"display": true,					"editable": true,					"disableRefresh": false				},				{					"name": "k",					"value": "3",					"widget": "textfield",					"title": "K",					"description": "The number of principal components",					"datatypes": [						"integer"					],					"required": true,					"display": true,					"editable": true,					"disableRefresh": false				}			],			"engine": "scala"		},		{			"id": "20",			"name": "PrintNRows",			"description": "Prints the specified number of records in the DataFrame. It is useful for seeing intermediate output",			"details": "This node is used to print incoming dataset.<br>\n<br>\nNumber of rows that needs to be printed can be configured in the node.<br>",			"examples": "",			"type": "transform",			"nodeClass": "fire.nodes.util.NodePrintFirstNRows",			"x": "674px",			"y": "337px",			"fields": [				{					"name": "storageLevel",					"value": "DEFAULT",					"widget": "array",					"title": "Output Storage Level",					"description": "Storage Level of the Output Datasets of this Node",					"optionsArray": [						"DEFAULT",						"NONE",						"DISK_ONLY",						"DISK_ONLY_2",						"MEMORY_ONLY",						"MEMORY_ONLY_2",						"MEMORY_ONLY_SER",						"MEMORY_ONLY_SER_2",						"MEMORY_AND_DISK",						"MEMORY_AND_DISK_2",						"MEMORY_AND_DISK_SER",						"MEMORY_AND_DISK_SER_2",						"OFF_HEAP"					],					"required": false,					"display": true,					"editable": true,					"disableRefresh": false				},				{					"name": "title",					"value": "Row Values",					"widget": "textfield",					"title": "Title",					"required": false,					"display": true,					"editable": true,					"disableRefresh": false				},				{					"name": "n",					"value": "10",					"widget": "textfield",					"title": "Num Rows to Print",					"description": "number of rows to be printed",					"required": false,					"display": true,					"editable": true,					"disableRefresh": false				},				{					"name": "displayDataType",					"value": "true",					"widget": "array",					"title": "Display Data Type",					"description": "If true display rows DataType",					"optionsArray": [						"true",						"false"					],					"required": false,					"display": true,					"editable": true,					"disableRefresh": false				}			],			"engine": "all"		},		{			"id": "21",			"name": "Sticky Note",			"description": "Allows capturing Notes on the Workflow",			"details": "",			"examples": "",			"type": "sticky",			"nodeClass": "fire.nodes.doc.NodeStickyNote",			"x": "131px",			"y": "411px",			"fields": [				{					"name": "storageLevel",					"value": "DEFAULT",					"widget": "array",					"title": "Output Storage Level",					"description": "Storage Level of the Output Datasets of this Node",					"optionsArray": [						"DEFAULT",						"NONE",						"DISK_ONLY",						"DISK_ONLY_2",						"MEMORY_ONLY",						"MEMORY_ONLY_2",						"MEMORY_ONLY_SER",						"MEMORY_ONLY_SER_2",						"MEMORY_AND_DISK",						"MEMORY_AND_DISK_2",						"MEMORY_AND_DISK_SER",						"MEMORY_AND_DISK_SER_2",						"OFF_HEAP"					],					"required": false,					"display": true,					"editable": true,					"disableRefresh": false				},				{					"name": "bgColor",					"value": "gray",					"widget": "textfield",					"title": "Bg Color",					"description": "Background of note",					"required": false,					"display": true,					"editable": true,					"disableRefresh": false				},				{					"name": "width",					"value": "246px",					"widget": "textfield",					"title": "Width",					"description": "Width of note",					"required": false,					"display": true,					"editable": true,					"disableRefresh": false				},				{					"name": "height",					"value": "67px",					"widget": "textfield",					"title": "Height",					"description": "Height of note",					"required": false,					"display": true,					"editable": true,					"disableRefresh": false				},				{					"name": "comment",					"value": "<p>Flag and remove outliers from price data</p>",					"widget": "textarea_rich",					"title": "Comment",					"description": "Comments for the Workflow",					"required": false,					"display": true,					"editable": true,					"disableRefresh": false				}			],			"engine": "all"		},		{			"id": "22",			"name": "Sticky Note",			"description": "Allows capturing Notes on the Workflow",			"details": "",			"examples": "",			"type": "sticky",			"nodeClass": "fire.nodes.doc.NodeStickyNote",			"x": "445px",			"y": "208px",			"fields": [				{					"name": "storageLevel",					"value": "DEFAULT",					"widget": "array",					"title": "Output Storage Level",					"description": "Storage Level of the Output Datasets of this Node",					"optionsArray": [						"DEFAULT",						"NONE",						"DISK_ONLY",						"DISK_ONLY_2",						"MEMORY_ONLY",						"MEMORY_ONLY_2",						"MEMORY_ONLY_SER",						"MEMORY_ONLY_SER_2",						"MEMORY_AND_DISK",						"MEMORY_AND_DISK_2",						"MEMORY_AND_DISK_SER",						"MEMORY_AND_DISK_SER_2",						"OFF_HEAP"					],					"required": false,					"display": true,					"editable": true,					"disableRefresh": false				},				{					"name": "bgColor",					"value": "gray",					"widget": "textfield",					"title": "Bg Color",					"description": "Background of note",					"required": false,					"display": true,					"editable": true,					"disableRefresh": false				},				{					"name": "width",					"value": "179px",					"widget": "textfield",					"title": "Width",					"description": "Width of note",					"required": false,					"display": true,					"editable": true,					"disableRefresh": false				},				{					"name": "height",					"value": "50px",					"widget": "textfield",					"title": "Height",					"description": "Height of note",					"required": false,					"display": true,					"editable": true,					"disableRefresh": false				},				{					"name": "comment",					"value": "<p>Assemble vector for PCA</p>",					"widget": "textarea_rich",					"title": "Comment",					"description": "Comments for the Workflow",					"required": false,					"display": true,					"editable": true,					"disableRefresh": false				}			],			"engine": "all"		},		{			"id": "23",			"name": "Sticky Note",			"description": "Allows capturing Notes on the Workflow",			"details": "",			"examples": "",			"type": "sticky",			"nodeClass": "fire.nodes.doc.NodeStickyNote",			"x": "458px",			"y": "424px",			"fields": [				{					"name": "storageLevel",					"value": "DEFAULT",					"widget": "array",					"title": "Output Storage Level",					"description": "Storage Level of the Output Datasets of this Node",					"optionsArray": [						"DEFAULT",						"NONE",						"DISK_ONLY",						"DISK_ONLY_2",						"MEMORY_ONLY",						"MEMORY_ONLY_2",						"MEMORY_ONLY_SER",						"MEMORY_ONLY_SER_2",						"MEMORY_AND_DISK",						"MEMORY_AND_DISK_2",						"MEMORY_AND_DISK_SER",						"MEMORY_AND_DISK_SER_2",						"OFF_HEAP"					],					"required": false,					"display": true,					"editable": true,					"disableRefresh": false				},				{					"name": "bgColor",					"value": "gray",					"widget": "textfield",					"title": "Bg Color",					"description": "Background of note",					"required": false,					"display": true,					"editable": true,					"disableRefresh": false				},				{					"name": "width",					"value": "171px",					"widget": "textfield",					"title": "Width",					"description": "Width of note",					"required": false,					"display": true,					"editable": true,					"disableRefresh": false				},				{					"name": "height",					"value": "78px",					"widget": "textfield",					"title": "Height",					"description": "Height of note",					"required": false,					"display": true,					"editable": true,					"disableRefresh": false				},				{					"name": "comment",					"value": "<p>Encode string values into numeric values</p>",					"widget": "textarea_rich",					"title": "Comment",					"description": "Comments for the Workflow",					"required": false,					"display": true,					"editable": true,					"disableRefresh": false				}			],			"engine": "all"		},		{			"id": "24",			"name": "Sticky Note",			"description": "Allows capturing Notes on the Workflow",			"details": "",			"examples": "",			"type": "sticky",			"nodeClass": "fire.nodes.doc.NodeStickyNote",			"x": "653px",			"y": "54px",			"fields": [				{					"name": "storageLevel",					"value": "DEFAULT",					"widget": "array",					"title": "Output Storage Level",					"description": "Storage Level of the Output Datasets of this Node",					"optionsArray": [						"DEFAULT",						"NONE",						"DISK_ONLY",						"DISK_ONLY_2",						"MEMORY_ONLY",						"MEMORY_ONLY_2",						"MEMORY_ONLY_SER",						"MEMORY_ONLY_SER_2",						"MEMORY_AND_DISK",						"MEMORY_AND_DISK_2",						"MEMORY_AND_DISK_SER",						"MEMORY_AND_DISK_SER_2",						"OFF_HEAP"					],					"required": false,					"display": true,					"editable": true,					"disableRefresh": false				},				{					"name": "bgColor",					"value": "gray",					"widget": "textfield",					"title": "Bg Color",					"description": "Background of note",					"required": false,					"display": true,					"editable": true,					"disableRefresh": false				},				{					"name": "width",					"value": "252px",					"widget": "textfield",					"title": "Width",					"description": "Width of note",					"required": false,					"display": true,					"editable": true,					"disableRefresh": false				},				{					"name": "height",					"value": "76px",					"widget": "textfield",					"title": "Height",					"description": "Height of note",					"required": false,					"display": true,					"editable": true,					"disableRefresh": false				},				{					"name": "comment",					"value": "<p>Conduct Principle Component Analysis for dimension reduction</p>",					"widget": "textarea_rich",					"title": "Comment",					"description": "Comments for the Workflow",					"required": false,					"display": true,					"editable": true,					"disableRefresh": false				}			],			"engine": "all"		}	],	"edges": [		{			"source": "13",			"target": "14",			"id": 1		},		{			"source": "14",			"target": "12",			"id": 2		},		{			"source": "15",			"target": "13",			"id": 3		},		{			"source": "12",			"target": "18",			"id": 4		},		{			"source": "18",			"target": "2",			"id": 5		},		{			"source": "2",			"target": "19",			"id": 6		},		{			"source": "19",			"target": "20",			"id": 7		}	],	"dataSetDetails": [],	"engine": "scala"}