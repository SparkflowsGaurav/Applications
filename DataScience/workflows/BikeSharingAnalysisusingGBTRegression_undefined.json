{	"name": "Bike Sharing Analysis using GBT Regression",	"uuid": "164e7b44-2baa-42b7-b772-effab49c3cfd",	"category": "Regression",	"description": "Predicts the number of bikes to be rented in any given hour",	"nodes": [		{			"id": "1",			"name": "DatasetStructured",			"description": "This Node creates a DataFrame by reading data from HDFS, HIVE etc. The dataset has been defined earlier in Fire by using the Dataset Feature. As a user, you just have to select the Dataset of your interest.",			"details": "This Node creates a DataFrame by reading data from HDFS, HIVE etc.<br>\n<br>\nThe data has been defined earlier in Fire by using the Dataset Feature. As a user, you just have to select the Dataset of your interest.<br>",			"examples": "",			"type": "dataset",			"nodeClass": "fire.nodes.dataset.NodeDatasetStructured",			"x": "44px",			"y": "199.625px",			"fields": [				{					"name": "storageLevel",					"value": "DEFAULT",					"widget": "array",					"title": "Output Storage Level",					"description": "Storage Level of the Output Datasets of this Node",					"optionsArray": [						"DEFAULT",						"NONE",						"DISK_ONLY",						"DISK_ONLY_2",						"MEMORY_ONLY",						"MEMORY_ONLY_2",						"MEMORY_ONLY_SER",						"MEMORY_ONLY_SER_2",						"MEMORY_AND_DISK",						"MEMORY_AND_DISK_2",						"MEMORY_AND_DISK_SER",						"MEMORY_AND_DISK_SER_2",						"OFF_HEAP"					],					"required": false,					"display": true,					"editable": true,					"disableRefresh": false				},				{					"name": "dataset",					"value": "d41de163-42b1-4d60-b7df-2d45eac03710",					"widget": "dataset",					"title": "Dataset",					"description": "Selected Dataset",					"required": true,					"display": true,					"editable": true,					"disableRefresh": false				}			],			"engine": "scala"		},		{			"id": "2",			"name": "Extract Hour from Time",			"description": "It creates a new DataFrame by extracting Date and Time fields.",			"details": "It creates a new DataFrame by extracting Date and Time fields.<br>\n<br>\nThe output DataFrame has year/month/dayofmonth/hour/minute/second values extracted from the specified TimeStamp column into new columns<br>",			"examples": "If incoming Dataframe has Date value as 2018-01-01 14:30:45 in YYYY-MM-DD HH:mm:ss format then using datetimeextract node would result in followings <br>\nadded as new columns to the Dataframe:<br>\n<br>\n<ul>\n<li> YEAR : 2018 </li>\n<li> MONTH : 01</li>\n<li> DAY OF MONTH : 01 </li>\n<li> HOUR : 14</li>\n<li> MINUTE : 30</li>\n<li> SECOND : 45</li>\n<li> WEEKOFYEAR : 1</li>\n</ul>",			"type": "transform",			"nodeClass": "fire.nodes.etl.NodeDateTimeFieldExtract",			"x": "246px",			"y": "205.625px",			"fields": [				{					"name": "storageLevel",					"value": "DEFAULT",					"widget": "array",					"title": "Output Storage Level",					"description": "Storage Level of the Output Datasets of this Node",					"optionsArray": [						"DEFAULT",						"NONE",						"DISK_ONLY",						"DISK_ONLY_2",						"MEMORY_ONLY",						"MEMORY_ONLY_2",						"MEMORY_ONLY_SER",						"MEMORY_ONLY_SER_2",						"MEMORY_AND_DISK",						"MEMORY_AND_DISK_2",						"MEMORY_AND_DISK_SER",						"MEMORY_AND_DISK_SER_2",						"OFF_HEAP"					],					"required": false,					"display": true,					"editable": true,					"disableRefresh": false				},				{					"name": "inputCol",					"value": "datetime",					"widget": "variable",					"title": "Column",					"description": "The input column name",					"datatypes": [						"date",						"timestamp"					],					"required": true,					"display": true,					"editable": true,					"disableRefresh": false				},				{					"name": "extractYear",					"value": "true",					"widget": "array",					"title": "Extract Year",					"description": "Extract Year",					"datatypes": [						"boolean"					],					"optionsArray": [						"true",						"false"					],					"required": true,					"display": true,					"editable": true,					"disableRefresh": false				},				{					"name": "extractMonth",					"value": "true",					"widget": "array",					"title": "Extract Month",					"description": "Extract Month",					"datatypes": [						"boolean"					],					"optionsArray": [						"true",						"false"					],					"required": true,					"display": true,					"editable": true,					"disableRefresh": false				},				{					"name": "extractDayOfMonth",					"value": "true",					"widget": "array",					"title": "Extract Day of Month",					"description": "Extract Day of Month",					"datatypes": [						"boolean"					],					"optionsArray": [						"true",						"false"					],					"required": true,					"display": true,					"editable": true,					"disableRefresh": false				},				{					"name": "extractHour",					"value": "true",					"widget": "array",					"title": "Extract Hour",					"description": "Extract Hour",					"datatypes": [						"boolean"					],					"optionsArray": [						"true",						"false"					],					"required": true,					"display": true,					"editable": true,					"disableRefresh": false				},				{					"name": "extractMinute",					"value": "false",					"widget": "array",					"title": "Extract Minute",					"description": "Extract Minute",					"datatypes": [						"boolean"					],					"optionsArray": [						"true",						"false"					],					"required": true,					"display": true,					"editable": true,					"disableRefresh": false				},				{					"name": "extractSecond",					"value": "false",					"widget": "array",					"title": "Extract Second",					"description": "Extract Second",					"datatypes": [						"boolean"					],					"optionsArray": [						"true",						"false"					],					"required": true,					"display": true,					"editable": true,					"disableRefresh": false				},				{					"name": "extractWeekOfYear",					"value": "false",					"widget": "array",					"title": "Extract WeekOfYear",					"description": "Extract WeekOfYear",					"datatypes": [						"boolean"					],					"optionsArray": [						"true",						"false"					],					"required": true,					"display": true,					"editable": true,					"disableRefresh": false				}			],			"engine": "scala"		},		{			"id": "3",			"name": "Cast Count to Double",			"description": "This node creates a new DataFrame by casting the specified input columns to a new data type",			"details": "This node creates a new DataFrame by casting the specified input columns to a new data type. All the selected columns would be cast to the specified data type.<br>\n<br>\nThe boolean field Replace Existing Columns indicates whether the existing column should be replaced or a new column should be created.<br>",			"examples": "If incoming Dataframe has following columns with below specified datatype:<br>\n<br>\n<ul>\n<li> CUST_ID : Integer</li>\n<li> CUST_NAME : String</li>\n<li> DOB : Datetime</li>\n<li> AGE : Integer</li>\n</ul>\nand [DOB] and [AGE] are selected for casting to [STRING] datatype then outgoing Dataframe would have below datatypes:<br>\n<br>\n<ul>\n<li> CUST_ID : Integer</li>\n<li> CUST_NAME : String</li>\n<li> DOB : String</li>\n<li> AGE : String</li>\n</ul>",			"type": "transform",			"nodeClass": "fire.nodes.etl.NodeCastColumnType",			"x": "453px",			"y": "398.625px",			"fields": [				{					"name": "storageLevel",					"value": "DEFAULT",					"widget": "array",					"title": "Output Storage Level",					"description": "Storage Level of the Output Datasets of this Node",					"optionsArray": [						"DEFAULT",						"NONE",						"DISK_ONLY",						"DISK_ONLY_2",						"MEMORY_ONLY",						"MEMORY_ONLY_2",						"MEMORY_ONLY_SER",						"MEMORY_ONLY_SER_2",						"MEMORY_AND_DISK",						"MEMORY_AND_DISK_2",						"MEMORY_AND_DISK_SER",						"MEMORY_AND_DISK_SER_2",						"OFF_HEAP"					],					"required": false,					"display": true,					"editable": true,					"disableRefresh": false				},				{					"name": "inputCols",					"value": "[\"count\"]",					"widget": "variables",					"title": "Columns",					"description": "Columns to be cast to new data type",					"required": true,					"display": true,					"editable": true,					"disableRefresh": false				},				{					"name": "outputColType",					"value": "DOUBLE",					"widget": "array",					"title": "New Data Type",					"description": "New data type for the selected columns (INTEGER, DOUBLE, STRING, LONG, SHORT)",					"optionsArray": [						"BOOLEAN",						"BYTE",						"DATE",						"DECIMAL",						"DOUBLE",						"FLOAT",						"INTEGER",						"LONG",						"SHORT",						"STRING",						"TIMESTAMP"					],					"required": true,					"display": true,					"editable": true,					"disableRefresh": false				},				{					"name": "replaceExistingCols",					"value": "true",					"widget": "array",					"title": "Replace Existing Cols?",					"description": "Whether to replace existing columns or create new ones?",					"optionsArray": [						"true",						"false"					],					"required": true,					"display": true,					"editable": true,					"disableRefresh": false				}			],			"engine": "scala"		},		{			"id": "4",			"name": "Assemble Features for Modeling",			"description": "Merges multiple columns into a vector column",			"details": "VectorAssembler is a transformer that combines a given list of columns into a single vector column. <br>\nIt is useful for combining raw features and features generated by different feature transformers into a single feature vector, in order to train ML models like logistic regression and decision trees. <br>\nVectorAssembler accepts the following input column types: all numeric types, boolean type, and vector type. In each row, the values of the input columns will be concatenated into a vector in the specified order.<br>\n<br>\nMore details are available at:<br>\n<br>\n<a href=\"https://spark.apache.org/docs/latest/ml-features.html#vectorassembler\" target=\"_blank\">spark.apache.org/docs/latest/ml-features.html#vectorassembler</a><br>",			"examples": "<h2>The below example is available at : <a href=\"https://spark.apache.org/docs/latest/ml-features.html#vectorassembler\" target=\"_blank\">spark.apache.org/docs/latest/ml-features.html#vectorassembler</a></h2>\n<br>\nimport org.apache.spark.ml.feature.VectorAssembler<br>\nimport org.apache.spark.ml.linalg.Vectors<br>\n<br>\nval dataset = spark.createDataFrame(<br>\n  Seq((0, 18, 1.0, Vectors.dense(0.0, 10.0, 0.5), 1.0))<br>\n).toDF(\"id\", \"hour\", \"mobile\", \"userFeatures\", \"clicked\")<br>\n<br>\nval assembler = new VectorAssembler()<br>\n  .setInputCols(Array(\"hour\", \"mobile\", \"userFeatures\"))<br>\n  .setOutputCol(\"features\")<br>\n<br>\nval output = assembler.transform(dataset)<br>\nprintln(\"Assembled columns 'hour', 'mobile', 'userFeatures' to vector column 'features'\")<br>\noutput.select(\"features\", \"clicked\").show(false)<br>",			"type": "ml-transformer",			"nodeClass": "fire.nodes.ml.NodeVectorAssembler",			"x": "598px",			"y": "183.613px",			"fields": [				{					"name": "storageLevel",					"value": "DEFAULT",					"widget": "array",					"title": "Output Storage Level",					"description": "Storage Level of the Output Datasets of this Node",					"optionsArray": [						"DEFAULT",						"NONE",						"DISK_ONLY",						"DISK_ONLY_2",						"MEMORY_ONLY",						"MEMORY_ONLY_2",						"MEMORY_ONLY_SER",						"MEMORY_ONLY_SER_2",						"MEMORY_AND_DISK",						"MEMORY_AND_DISK_2",						"MEMORY_AND_DISK_SER",						"MEMORY_AND_DISK_SER_2",						"OFF_HEAP"					],					"required": false,					"display": true,					"editable": true,					"disableRefresh": false				},				{					"name": "inputCols",					"value": "[\"season\",\"holiday\",\"workingday\",\"weather\",\"humidity\",\"datetime_year\",\"datetime_month\",\"datetime_dayofmonth\",\"datetime_hour\",\"temp\",\"atemp\",\"windspeed\"]",					"widget": "variables",					"title": "Input Columns",					"description": "Input column of type - all numeric, boolean and vector",					"datatypes": [						"integer",						"long",						"double",						"float",						"vectorudt"					],					"required": false,					"display": true,					"editable": true,					"disableRefresh": false				},				{					"name": "outputCol",					"value": "feature_vector",					"widget": "textfield",					"title": "Output Column",					"description": "Output column name",					"required": true,					"display": true,					"editable": true,					"disableRefresh": false				},				{					"name": "handleInvalid",					"value": "error",					"widget": "array",					"title": "HandleInvalid",					"description": "How to handle invalid data (NULL values). Options are 'skip' (filter out rows with invalid data), 'error' (throw an error), or 'keep' (return relevant number of NaN in the output).",					"optionsArray": [						"error",						"skip",						"keep"					],					"required": false,					"display": true,					"editable": true,					"disableRefresh": false				}			],			"engine": "scala"		},		{			"id": "5",			"name": "VectorIndexer",			"description": "Vector Indexer indexes categorical features inside of a Vector. It decides which features are categorical and converts them to category indices. The decision is based on the number of distinct values of a feature.",			"details": "VectorIndexer helps index categorical features in datasets of Vectors. It can both automatically decide which features are categorical and convert original values to category indices.<br>\nMore details are available at: <a href=\"https://spark.apache.org/docs/2.0.0/ml-features.html#vectorindexer\" target=\"_blank\">spark.apache.org/docs/2.0.0/ml-features.html#vectorindexer</a><br>",			"examples": "<h2>The below example is available at : <a href=\"https://spark.apache.org/docs/2.0.0/ml-features.html#vectorindexer\" target=\"_blank\">spark.apache.org/docs/2.0.0/ml-features.html#vectorindexer</a></h2>\n<br>\nimport org.apache.spark.ml.feature.VectorIndexer<br>\n<br>\nval data = spark.read.format(\"libsvm\").load(\"data/mllib/sample_libsvm_data.txt\")<br>\n<br>\nval indexer = new VectorIndexer()<br>\n  .setInputCol(\"features\")<br>\n  .setOutputCol(\"indexed\")<br>\n  .setMaxCategories(10)<br>\n<br>\nval indexerModel = indexer.fit(data)<br>\n<br>\nval categoricalFeatures: Set[Int] = indexerModel.categoryMaps.keys.toSet<br>\nprintln(s\"Chose ${categoricalFeatures.size} categorical features: \" +<br>\n  categoricalFeatures.mkString(\", \"))<br>\n<br>\n// Create new column \"indexed\" with categorical values transformed to indices<br>\nval indexedData = indexerModel.transform(data)<br>\nindexedData.show()<br>",			"type": "ml-transformer",			"nodeClass": "fire.nodes.ml.NodeVectorIndexer",			"x": "597.987px",			"y": "387.612px",			"fields": [				{					"name": "storageLevel",					"value": "DEFAULT",					"widget": "array",					"title": "Output Storage Level",					"description": "Storage Level of the Output Datasets of this Node",					"optionsArray": [						"DEFAULT",						"NONE",						"DISK_ONLY",						"DISK_ONLY_2",						"MEMORY_ONLY",						"MEMORY_ONLY_2",						"MEMORY_ONLY_SER",						"MEMORY_ONLY_SER_2",						"MEMORY_AND_DISK",						"MEMORY_AND_DISK_2",						"MEMORY_AND_DISK_SER",						"MEMORY_AND_DISK_SER_2",						"OFF_HEAP"					],					"required": false,					"display": true,					"editable": true,					"disableRefresh": false				},				{					"name": "inputCol",					"value": "feature_vector",					"widget": "variable",					"title": "Input Column",					"description": "The Input column name",					"datatypes": [						"vectorudt"					],					"required": false,					"display": true,					"editable": true,					"disableRefresh": false				},				{					"name": "outputCol",					"value": "feature_vector_index",					"widget": "textfield",					"title": "Output Column",					"description": "Output column name",					"datatypes": [						"vectorudt"					],					"required": true,					"display": true,					"editable": true,					"disableRefresh": false				},				{					"name": "maxCategories",					"value": "31",					"widget": "textfield",					"title": "Maximum Categories",					"description": "Threshold for the number of values a categorical feature can take. If a feature is found to have > maxCategories values, then it is declared continuous. Must be >= 2",					"datatypes": [						"integer"					],					"required": false,					"display": true,					"editable": true,					"disableRefresh": false				}			],			"engine": "scala"		},		{			"id": "6",			"name": "Split 80-20",			"description": "This node splits the incoming DataFrame into 2. It takes in the fraction to use in splitting the data. For example, if the fraction is .7, it would split the data into 2 DataFrames, one containing 70% of the rows(passed from lower edge id to next node) and the other containing the remaining 30%(passed from higher edge id to next node).",			"details": "This node splits the incoming DataFrame into 2. It takes in the fraction to use in splitting the data.<br>\n<br>\nFor example, if the fraction is .7, it would split the data into 2 DataFrames, one containing 70% of the rows(passed from lower edge id to next node) and the other containing the remaining 30%(passed from higher edge id to next node).<br>\n<br>\nThe split node can be used for splitting the DataFrame for training and test datasets used in Machine Learning.<br>",			"examples": "",			"type": "transform",			"nodeClass": "fire.nodes.ml.NodeSplit",			"x": "724.975px",			"y": "379.612px",			"fields": [				{					"name": "storageLevel",					"value": "DEFAULT",					"widget": "array",					"title": "Output Storage Level",					"description": "Storage Level of the Output Datasets of this Node",					"optionsArray": [						"DEFAULT",						"NONE",						"DISK_ONLY",						"DISK_ONLY_2",						"MEMORY_ONLY",						"MEMORY_ONLY_2",						"MEMORY_ONLY_SER",						"MEMORY_ONLY_SER_2",						"MEMORY_AND_DISK",						"MEMORY_AND_DISK_2",						"MEMORY_AND_DISK_SER",						"MEMORY_AND_DISK_SER_2",						"OFF_HEAP"					],					"required": false,					"display": true,					"editable": true,					"disableRefresh": false				},				{					"name": "fraction1",					"value": ".8",					"widget": "textfield",					"title": "Fraction 1",					"description": "Fraction to be used for Splitting the DataFrame into two. The first DataFrame would go to the lower edge output. The other would go to the higher edge output.",					"required": true,					"display": true,					"editable": true,					"disableRefresh": false				}			],			"engine": "scala"		},		{			"id": "7",			"name": "GBTRegression",			"description": "It supports both continuous and categorical features.",			"details": "Gradient-Boosted Trees (GBTs) are ensembles of decision trees. GBTs iteratively train decision trees in order to minimize a loss function. <br>\nThe spark.ml implementation supports GBTs for binary classification and for regression, using both continuous and categorical features.<br>\n<br>\nMore details are available at Apache Spark ML docs page:<br>\n<br>\n<a href=\"http://spark.apache.org/docs/latest/ml-classification-regression.html#gradient-boosted-tree-regression\" target=\"_blank\">spark.apache.org/docs/latest/ml-classification-regression.html#gradient-boosted-tree-regression</a><br>",			"examples": "Below example is available at : <a href=\"https://spark.apache.org/docs/latest/ml-classification-regression.html#gradient-boosted-tree-regression\" target=\"_blank\">spark.apache.org/docs/latest/ml-classification-regression.html#gradient-boosted-tree-regression</a><br>\n<br>\n<br>\nimport org.apache.spark.ml.Pipeline<br>\nimport org.apache.spark.ml.evaluation.RegressionEvaluator<br>\nimport org.apache.spark.ml.feature.VectorIndexer<br>\nimport org.apache.spark.ml.regression.{GBTRegressionModel, GBTRegressor}<br>\n<br>\n// Load and parse the data file, converting it to a DataFrame.<br>\nval data = spark.read.format(\"libsvm\").load(\"data/mllib/sample_libsvm_data.txt\")<br>\n<br>\n// Automatically identify categorical features, and index them.<br>\n// Set maxCategories so features with > 4 distinct values are treated as continuous.<br>\nval featureIndexer = new VectorIndexer()<br>\n  .setInputCol(\"features\")<br>\n  .setOutputCol(\"indexedFeatures\")<br>\n  .setMaxCategories(4)<br>\n  .fit(data)<br>\n<br>\n// Split the data into training and test sets (30% held out for testing).<br>\nval Array(trainingData, testData) = data.randomSplit(Array(0.7, 0.3))<br>\n<br>\n// Train a GBT model.<br>\nval gbt = new GBTRegressor()<br>\n  .setLabelCol(\"label\")<br>\n  .setFeaturesCol(\"indexedFeatures\")<br>\n  .setMaxIter(10)<br>\n<br>\n// Chain indexer and GBT in a Pipeline.<br>\nval pipeline = new Pipeline()<br>\n  .setStages(Array(featureIndexer, gbt))<br>\n<br>\n// Train model. This also runs the indexer.<br>\nval model = pipeline.fit(trainingData)<br>\n<br>\n// Make predictions.<br>\nval predictions = model.transform(testData)<br>\n<br>\n// Select example rows to display.<br>\npredictions.select(\"prediction\", \"label\", \"features\").show(5)<br>\n<br>\n// Select (prediction, true label) and compute test error.<br>\nval evaluator = new RegressionEvaluator()<br>\n  .setLabelCol(\"label\")<br>\n  .setPredictionCol(\"prediction\")<br>\n  .setMetricName(\"rmse\")<br>\nval rmse = evaluator.evaluate(predictions)<br>\nprintln(s\"Root Mean Squared Error (RMSE) on test data = $rmse\")<br>\n<br>\nval gbtModel = model.stages(1).asInstanceOf[GBTRegressionModel]<br>\nprintln(s\"Learned regression GBT model:\\n ${gbtModel.toDebugString}\")<br>",			"type": "ml-estimator",			"nodeClass": "fire.nodes.ml.NodeGBTRegression",			"x": "729.975px",			"y": "178.613px",			"fields": [				{					"name": "storageLevel",					"value": "DEFAULT",					"widget": "array",					"title": "Output Storage Level",					"description": "Storage Level of the Output Datasets of this Node",					"optionsArray": [						"DEFAULT",						"NONE",						"DISK_ONLY",						"DISK_ONLY_2",						"MEMORY_ONLY",						"MEMORY_ONLY_2",						"MEMORY_ONLY_SER",						"MEMORY_ONLY_SER_2",						"MEMORY_AND_DISK",						"MEMORY_AND_DISK_2",						"MEMORY_AND_DISK_SER",						"MEMORY_AND_DISK_SER_2",						"OFF_HEAP"					],					"required": false,					"display": true,					"editable": true,					"disableRefresh": false				},				{					"name": "featuresCol",					"value": "feature_vector_index",					"widget": "variable",					"title": "Features Column",					"description": "Features column of type vectorUDT for model fitting",					"datatypes": [						"vectorudt"					],					"required": false,					"display": true,					"editable": true,					"disableRefresh": false				},				{					"name": "labelCol",					"value": "count",					"widget": "variable",					"title": "Label Column",					"description": "The label column for model fitting",					"datatypes": [						"double"					],					"required": false,					"display": true,					"editable": true,					"disableRefresh": false				},				{					"name": "predictionCol",					"value": "",					"widget": "textfield",					"title": "Prediction Column",					"description": "The prediction column created during model scoring.",					"required": false,					"display": true,					"editable": true,					"disableRefresh": false				},				{					"name": "impurity",					"value": "variance",					"widget": "array",					"title": "Impurity",					"description": "The Criterion used for information gain calculation",					"optionsArray": [						"variance"					],					"required": false,					"display": true,					"editable": true,					"disableRefresh": false				},				{					"name": "lossType",					"value": "squared",					"widget": "array",					"title": "Loss Function",					"description": "The Loss function which GBT tries to minimize",					"optionsArray": [						"squared",						"absolute"					],					"required": false,					"display": true,					"editable": true,					"disableRefresh": false				},				{					"name": "maxBins",					"value": "32",					"widget": "textfield",					"title": "Max Bins",					"description": "The maximum number of bins used for discretizing continuous features.Must be >= 2 and >= number of categories in any categorical feature.",					"datatypes": [						"integer"					],					"required": false,					"display": true,					"editable": true,					"disableRefresh": false				},				{					"name": "maxDepth",					"value": "5",					"widget": "textfield",					"title": "Max Depth",					"description": "The Maximum depth of a tree",					"datatypes": [						"integer"					],					"required": false,					"display": true,					"editable": true,					"disableRefresh": false				},				{					"name": "maxIter",					"value": "5",					"widget": "textfield",					"title": "Max Iterations",					"description": "The maximum number of iterations(>=0)(a.k.a numtrees)",					"datatypes": [						"integer"					],					"required": false,					"display": true,					"editable": true,					"disableRefresh": false				},				{					"name": "minInfoGain",					"value": "0.0",					"widget": "textfield",					"title": "Min Information Gain",					"description": "The Minimum information gain for a split to be considered at a tree node",					"datatypes": [						"double"					],					"required": false,					"display": true,					"editable": true,					"disableRefresh": false				},				{					"name": "minInstancesPerNode",					"value": "1",					"widget": "textfield",					"title": "Min Instances Per Node",					"description": "The Minimum number of instances each child must have after split",					"datatypes": [						"integer"					],					"required": false,					"display": true,					"editable": true,					"disableRefresh": false				},				{					"name": "subsamplingRate",					"value": "1.0",					"widget": "textfield",					"title": "Subsampling Rate",					"description": "The fraction of the training data used for learning each decision tree.",					"datatypes": [						"double"					],					"required": false,					"display": true,					"editable": true,					"disableRefresh": false				},				{					"name": "seed",					"value": "",					"widget": "textfield",					"title": "Seed",					"description": "The random seed",					"datatypes": [						"long"					],					"required": false,					"display": true,					"editable": true,					"disableRefresh": false				},				{					"name": "stepSize",					"value": "0.1",					"widget": "textfield",					"title": "Step Size",					"description": "Step size (a.k.a. learning rate), The step size to be used for each iteration of optimization.",					"datatypes": [						"double"					],					"required": false,					"display": true,					"editable": true,					"disableRefresh": false				},				{					"name": "cacheNodeIds",					"value": "false",					"widget": "array",					"title": "Cache Node Ids",					"description": "The caching nodes IDs. Can speed up training of deeper trees.",					"datatypes": [						"boolean"					],					"optionsArray": [						"false",						"true"					],					"required": false,					"display": true,					"editable": true,					"disableRefresh": false				},				{					"name": "checkpointInterval",					"value": "10",					"widget": "textfield",					"title": "Checkpoint Interval",					"description": "The checkpoint interval. E.g. 10 means that the cache will get checkpointed every 10 iterations.Set checkpoint interval (>= 1) or disable checkpoint (-1)",					"datatypes": [						"integer"					],					"required": false,					"display": true,					"editable": true,					"disableRefresh": false				},				{					"name": "maxMemoryInMB",					"value": "256",					"widget": "textfield",					"title": "Max memory",					"description": "Maximum memory in MB allocated to histogram aggregation.",					"datatypes": [						"integer"					],					"required": false,					"display": true,					"editable": true,					"disableRefresh": false				},				{					"name": "gridSearch",					"value": "",					"widget": "tab",					"title": "Grid Search",					"required": false,					"display": true,					"editable": true,					"disableRefresh": false				},				{					"name": "minInfoGainGrid",					"value": "",					"widget": "textfield",					"title": "Min Info Gain Grid Search",					"description": "Min Info Gain Grid Search",					"required": false,					"display": true,					"editable": true,					"disableRefresh": false				},				{					"name": "maxBinsGrid",					"value": "",					"widget": "textfield",					"title": "Max Bins Grid Search",					"description": "Max Bins for Grid Search",					"required": false,					"display": true,					"editable": true,					"disableRefresh": false				},				{					"name": "maxDepthGrid",					"value": "",					"widget": "textfield",					"title": "Max Depth Grid Search",					"description": "Regularization Parameters for Grid Search",					"required": false,					"display": true,					"editable": true,					"disableRefresh": false				},				{					"name": "maxIterGrid",					"value": "",					"widget": "textfield",					"title": "Max Iterations Grid Search",					"description": "Max Iterations for Grid Search",					"required": false,					"display": true,					"editable": true,					"disableRefresh": false				}			],			"engine": "scala"		},		{			"id": "8",			"name": "Predict",			"description": "Predict node takes in a DataFrame and Model and makes predictions",			"details": "Predict node takes in a DataFrame and Model and makes predictions on the data using the Model.<br>",			"examples": "",			"type": "ml-predict",			"nodeClass": "fire.nodes.ml.NodePredict",			"x": "956px",			"y": "182.613px",			"fields": [				{					"name": "storageLevel",					"value": "DEFAULT",					"widget": "array",					"title": "Output Storage Level",					"description": "Storage Level of the Output Datasets of this Node",					"optionsArray": [						"DEFAULT",						"NONE",						"DISK_ONLY",						"DISK_ONLY_2",						"MEMORY_ONLY",						"MEMORY_ONLY_2",						"MEMORY_ONLY_SER",						"MEMORY_ONLY_SER_2",						"MEMORY_AND_DISK",						"MEMORY_AND_DISK_2",						"MEMORY_AND_DISK_SER",						"MEMORY_AND_DISK_SER_2",						"OFF_HEAP"					],					"required": false,					"display": true,					"editable": true,					"disableRefresh": false				}			],			"engine": "scala"		},		{			"id": "9",			"name": "RegressionEvaluator",			"description": "Evaluator for regression, which expects two input columns: prediction and label.",			"details": "Evaluator for regression, which expects two input columns: prediction and label.<br>\n<br>\nMore details are available at Apache Spark ML docs page:<br>\n<br>\n<a href=\"https://spark.apache.org/docs/latest/api/java/org/apache/spark/ml/evaluation/RegressionEvaluator.html\" target=\"_blank\">spark.apache.org/docs/latest/api/java/org/apache/spark/ml/evaluation/RegressionEvaluator.html</a><br>",			"examples": "",			"type": "ml-evaluator",			"nodeClass": "fire.nodes.ml.NodeRegressionEvaluator",			"x": "960.65px",			"y": "353.638px",			"fields": [				{					"name": "storageLevel",					"value": "DEFAULT",					"widget": "array",					"title": "Output Storage Level",					"description": "Storage Level of the Output Datasets of this Node",					"optionsArray": [						"DEFAULT",						"NONE",						"DISK_ONLY",						"DISK_ONLY_2",						"MEMORY_ONLY",						"MEMORY_ONLY_2",						"MEMORY_ONLY_SER",						"MEMORY_ONLY_SER_2",						"MEMORY_AND_DISK",						"MEMORY_AND_DISK_2",						"MEMORY_AND_DISK_SER",						"MEMORY_AND_DISK_SER_2",						"OFF_HEAP"					],					"required": false,					"display": true,					"editable": true,					"disableRefresh": false				},				{					"name": "labelCol",					"value": "count",					"widget": "variable",					"title": "Label Column",					"description": "The label column for model fitting.",					"datatypes": [						"double"					],					"required": false,					"display": true,					"editable": true,					"disableRefresh": false				},				{					"name": "predictionCol",					"value": "prediction",					"widget": "variable",					"title": "Prediction Column",					"description": "The prediction column.",					"datatypes": [						"double"					],					"required": false,					"display": true,					"editable": true,					"disableRefresh": false				},				{					"name": "metricName",					"value": "rmse",					"widget": "array",					"title": "Metric Name",					"description": "The metric used in evaluation.",					"optionsArray": [						"rmse",						"mse",						"r2",						"mae"					],					"required": false,					"display": true,					"editable": true,					"disableRefresh": false				}			],			"engine": "scala"		},		{			"id": "10",			"name": "Count of Rentals per Hour",			"description": "This node runs the given SQL on the incoming DataFrame",			"details": "This node receives an input data frame, creates a temporary table on top of that data frame.<br>\nAllows the user to write a SQL which would be executed on the temporary table.<br>\nThe resulting data frame of running the SQL is passed on to the next node.<br>",			"examples": "<h2>SQL Examples</h2>\n<br>\nBelow are some example of SQL. <br>\n<br>\nThey use the Temp Table Name to be : tempTable<br>\n<br>\nThe schem of the Input Dataframe is : id, price, lotsize, bedrooms, bathrms, stories, driveway, recroom, fullbase, gashw, airco, garagepl, prefarea<br>\n<br>\n<h4> avg price of house</h4>\n<br>\nselect avg(price) as avg_price from tempTable<br>\n<br>\n<br>\n<h4> bedrooms with avg price greater than 10000</h4>\n<br>\nselect bedrooms, avg_price from<br>\n(select bedrooms, avg(price) as avg_price from tempTable group by bedrooms) as temp where avg_price > 10000<br>\n<br>\n<br>\n<h4> house details with bedrooms avg price greater than 10000</h4>\n<br>\nselect tempTable.* , inner_table.avg_price from<br>\n(select bedrooms, avg_price from<br>\n(select bedrooms, avg(price) as avg_price from tempTable group by bedrooms) as temp where avg_price > 10000) as inner_table<br>\nJOIN tempTable ON(inner_table.bedrooms = tempTable.bedrooms)<br>",			"type": "transform",			"nodeClass": "fire.nodes.etl.NodeSQL",			"x": "238.988px",			"y": "348.625px",			"hint": "Whenever the table is changed, go to Schema tab and Refresh the Schema",			"fields": [				{					"name": "storageLevel",					"value": "DEFAULT",					"widget": "array",					"title": "Output Storage Level",					"description": "Storage Level of the Output Datasets of this Node",					"optionsArray": [						"DEFAULT",						"NONE",						"DISK_ONLY",						"DISK_ONLY_2",						"MEMORY_ONLY",						"MEMORY_ONLY_2",						"MEMORY_ONLY_SER",						"MEMORY_ONLY_SER_2",						"MEMORY_AND_DISK",						"MEMORY_AND_DISK_2",						"MEMORY_AND_DISK_SER",						"MEMORY_AND_DISK_SER_2",						"OFF_HEAP"					],					"required": false,					"display": true,					"editable": true,					"disableRefresh": false				},				{					"name": "tempTable",					"value": "fire_temp_table",					"widget": "textfield",					"title": "Temp Table",					"description": "Temp Table Name to be used",					"required": false,					"display": true,					"editable": true,					"disableRefresh": false				},				{					"name": "sql",					"value": "select datetime_hour, sum(count) as count from fire_temp_table group by datetime_hour order by datetime_hour",					"widget": "textarea_large",					"type": "sql",					"title": "SQL",					"description": "SQL to be run",					"required": true,					"display": true,					"editable": true,					"disableRefresh": false				},				{					"name": "schema",					"value": "",					"widget": "tab",					"title": "Schema",					"required": false,					"display": true,					"editable": true,					"disableRefresh": false				},				{					"name": "outputColNames",					"value": "[\"datetime_hour\",\"count\"]",					"widget": "schema_col_names",					"title": "Output Column Names",					"description": "Name of the Output Columns",					"required": false,					"display": true,					"editable": true,					"disableRefresh": false				},				{					"name": "outputColTypes",					"value": "[\"INTEGER\",\"LONG\"]",					"widget": "schema_col_types",					"title": "Output Column Types",					"description": "Data Type of the Output Columns",					"required": false,					"display": true,					"editable": true,					"disableRefresh": false				},				{					"name": "outputColFormats",					"value": "[\"\",\"\"]",					"widget": "schema_col_formats",					"title": "Output Column Formats",					"description": "Format of the Output Columns",					"required": false,					"display": true,					"editable": true,					"disableRefresh": false				}			],			"engine": "scala"		},		{			"id": "11",			"name": "Graph Count of Rentals",			"details": "This node represents variation between data of various data series in Graphical format.<br>\n<br>\nMultiple numeric columns can be plotted along Y-Coordinate. Only one can be selected along X-Coordinate.<br>\n<br>\nVariation between dataseries is plotted as graph of the selected type such as Line-Chart, Bar-Chart or so on.<br>",			"examples": "",			"type": "transform",			"nodeClass": "fire.nodes.graph.NodeGraphValues",			"x": "240.988px",			"y": "502.625px",			"fields": [				{					"name": "storageLevel",					"value": "DEFAULT",					"widget": "array",					"title": "Output Storage Level",					"description": "Storage Level of the Output Datasets of this Node",					"optionsArray": [						"DEFAULT",						"NONE",						"DISK_ONLY",						"DISK_ONLY_2",						"MEMORY_ONLY",						"MEMORY_ONLY_2",						"MEMORY_ONLY_SER",						"MEMORY_ONLY_SER_2",						"MEMORY_AND_DISK",						"MEMORY_AND_DISK_2",						"MEMORY_AND_DISK_SER",						"MEMORY_AND_DISK_SER_2",						"OFF_HEAP"					],					"required": false,					"display": true,					"editable": true,					"disableRefresh": false				},				{					"name": "title",					"value": "Count of Rentals per Hour",					"widget": "textfield",					"title": "Title",					"required": false,					"display": true,					"editable": true,					"disableRefresh": false				},				{					"name": "xlabel",					"value": "X axis",					"widget": "textfield",					"title": "X Label",					"required": false,					"display": true,					"editable": true,					"disableRefresh": false				},				{					"name": "ylabel",					"value": "Y axis",					"widget": "textfield",					"title": "Y Label",					"required": false,					"display": true,					"editable": true,					"disableRefresh": false				},				{					"name": "maxValuesToDisplay",					"value": "20",					"widget": "textfield",					"title": "Max Values To Display",					"description": "Maximum number of values to display in result.",					"required": true,					"display": true,					"editable": true,					"disableRefresh": false				},				{					"name": "graphType",					"value": "LINECHART",					"widget": "enum",					"title": "Chart Type",					"optionsMap": {						"LINECHART": "Line Chart",						"COLUMNCHART": "Side by Side Bar Chart",						"BARCHART": "Stacked Bar Chart",						"PIE": "Pie Chart"					},					"required": false,					"display": true,					"editable": true,					"disableRefresh": false				},				{					"name": "chartColors",					"value": "",					"widget": "colors",					"title": "Chart Colors",					"required": false,					"display": true,					"editable": true,					"disableRefresh": false				},				{					"name": "isStreaming",					"value": "false",					"widget": "array",					"title": "Is Streaming?",					"description": "Whether the Graph is a Streaming Graph or not",					"datatypes": [						"boolean"					],					"optionsArray": [						"true",						"false"					],					"required": false,					"display": true,					"editable": true,					"disableRefresh": false				},				{					"name": "xCol",					"value": "datetime_hour",					"widget": "variable",					"title": "X Column",					"required": false,					"display": true,					"editable": true,					"disableRefresh": false				},				{					"name": "yCols",					"value": "[\"count\"]",					"widget": "variables",					"title": "Y Columns",					"datatypes": [						"integer",						"long",						"double",						"float"					],					"required": false,					"display": true,					"editable": true,					"disableRefresh": false				}			],			"engine": "scala"		},		{			"id": "13",			"name": "Correlation",			"description": "calculates the correlation between two series of data.",			"details": "This node calculates the correlation between two series of data in a common operation in Statistics.<br>\n<br>\nMore at Spark MLlib/ML docs page : <a href=\"http://spark.apache.org/docs/latest/mllib-statistics.html#correlations\" target=\"_blank\">spark.apache.org/docs/latest/mllib-statistics.html#correlations</a><br>",			"examples": "",			"type": "transform",			"nodeClass": "fire.nodes.ml.NodeCorrelation",			"x": "53.9625px",			"y": "351.637px",			"fields": [				{					"name": "storageLevel",					"value": "DEFAULT",					"widget": "array",					"title": "Output Storage Level",					"description": "Storage Level of the Output Datasets of this Node",					"optionsArray": [						"DEFAULT",						"NONE",						"DISK_ONLY",						"DISK_ONLY_2",						"MEMORY_ONLY",						"MEMORY_ONLY_2",						"MEMORY_ONLY_SER",						"MEMORY_ONLY_SER_2",						"MEMORY_AND_DISK",						"MEMORY_AND_DISK_2",						"MEMORY_AND_DISK_SER",						"MEMORY_AND_DISK_SER_2",						"OFF_HEAP"					],					"required": false,					"display": true,					"editable": true,					"disableRefresh": false				},				{					"name": "title",					"value": "Correlation between various Columns",					"widget": "textfield",					"title": "Title",					"required": false,					"display": true,					"editable": true,					"disableRefresh": false				},				{					"name": "inputCols",					"value": "[\"season\",\"holiday\",\"workingday\",\"weather\",\"humidity\",\"casual\",\"registered\",\"count\",\"temp\",\"atemp\",\"windspeed\"]",					"widget": "variables",					"title": "Input Column for Correlation",					"description": "Column Names to check correlation ",					"datatypes": [						"integer",						"long",						"double",						"float"					],					"required": false,					"display": true,					"editable": true,					"disableRefresh": false				}			],			"engine": "scala"		},		{			"id": "14",			"name": "Summary",			"description": "Summary statistics provide useful information about sample data. eg: measures of spread.",			"details": "Summary statistics provides useful information about sample data. eg: measures of spread.<br>\n<br>\nMore at Spark MLlib/ML docs page : <a href=\"http://spark.apache.org/docs/latest/mllib-statistics.html#summary-statistics\" target=\"_blank\">spark.apache.org/docs/latest/mllib-statistics.html#summary-statistics</a><br>\n<br>\nSummary Node provides a table consist of informations such as number of non-null entries (count), mean, standard deviation, and minimum and maximum value for each numerical column.<br>",			"examples": "A set of columns can be selected to display Summary on.<br>\n<br>\nIf SummaryStatistics node is configured to compute Statistical Summary for [Salary]<br>\n<br>\nthen outgoing Dataframe would be created as below displaying Statistical Summary values:<br>\n<br>\n<ul>\n<li> count\t:\t8</li>\n<li> mean\t:\t11500</li>\n<li> min\t:\t10000</li>\n<li> 25_percentile\t:\t10000</li>\n<li> 50_percentile\t:\t11000</li>\n<li> 75_percentile\t:\t12000</li>\n<li> max\t:\t13000</li>\n<li> stdev\t:\t1195.229</li>\n<li> variance\t:\t1428571.429</li>\n</ul>",			"type": "transform",			"nodeClass": "fire.nodes.ml.NodeSummary",			"x": "54.975px",			"y": "495.45px",			"fields": [				{					"name": "storageLevel",					"value": "DEFAULT",					"widget": "array",					"title": "Output Storage Level",					"description": "Storage Level of the Output Datasets of this Node",					"optionsArray": [						"DEFAULT",						"NONE",						"DISK_ONLY",						"DISK_ONLY_2",						"MEMORY_ONLY",						"MEMORY_ONLY_2",						"MEMORY_ONLY_SER",						"MEMORY_ONLY_SER_2",						"MEMORY_AND_DISK",						"MEMORY_AND_DISK_2",						"MEMORY_AND_DISK_SER",						"MEMORY_AND_DISK_SER_2",						"OFF_HEAP"					],					"required": false,					"display": true,					"editable": true,					"disableRefresh": false				},				{					"name": "title",					"value": "Bike Sharing Dataset Summary",					"widget": "textfield",					"title": "Title",					"required": false,					"display": true,					"editable": true,					"disableRefresh": false				},				{					"name": "colNames",					"value": "[\"season\",\"holiday\",\"workingday\",\"weather\",\"humidity\",\"casual\",\"registered\",\"count\",\"temp\",\"atemp\",\"windspeed\"]",					"widget": "variables",					"title": "Column Names",					"description": "Column Names for Summary",					"datatypes": [						"integer",						"long",						"double",						"float"					],					"required": false,					"display": true,					"editable": true,					"disableRefresh": false				}			],			"engine": "scala"		},		{			"id": "15",			"name": "Sticky Note",			"description": "Allows capturing Notes on the Workflow",			"details": "",			"examples": "",			"type": "sticky",			"nodeClass": "fire.nodes.doc.NodeStickyNote",			"x": "12px",			"y": "8px",			"fields": [				{					"name": "storageLevel",					"value": "DEFAULT",					"widget": "array",					"title": "Output Storage Level",					"description": "Storage Level of the Output Datasets of this Node",					"optionsArray": [						"DEFAULT",						"NONE",						"DISK_ONLY",						"DISK_ONLY_2",						"MEMORY_ONLY",						"MEMORY_ONLY_2",						"MEMORY_ONLY_SER",						"MEMORY_ONLY_SER_2",						"MEMORY_AND_DISK",						"MEMORY_AND_DISK_2",						"MEMORY_AND_DISK_SER",						"MEMORY_AND_DISK_SER_2",						"OFF_HEAP"					],					"required": false,					"display": true,					"editable": true,					"disableRefresh": false				},				{					"name": "bgColor",					"value": "blue",					"widget": "textfield",					"title": "Bg Color",					"description": "Background of note",					"required": false,					"display": true,					"editable": true,					"disableRefresh": false				},				{					"name": "width",					"value": "982px",					"widget": "textfield",					"title": "Width",					"description": "Width of note",					"required": false,					"display": true,					"editable": true,					"disableRefresh": false				},				{					"name": "height",					"value": "114px",					"widget": "textfield",					"title": "Height",					"description": "Height of note",					"required": false,					"display": true,					"editable": true,					"disableRefresh": false				},				{					"name": "comment",					"value": "<h1>Analyzing a bike sharing dataset Using GBT Regression Model</h1><p><br></p><p>This workflow creates an GBT Regression model for predicting the number of bikes to be rented in any given day and hour. - Click note for more info</p><p><br></p><h2>Data</h2><p><br></p><p>The dataset contains bike rental info from 2011 and 2012 in the Capital bikeshare system, plus additional relevant information such as weather. This dataset is from Fanaee-T and Gama (2013) and is hosted by the UCI Machine Learning Repository.</p><p><br></p><p><br></p><h2>Goal</h2><p><br></p><p>We want to learn to predict bike rental counts (per hour) from information such as day of the week, weather, season, etc. Having good predictions of customer demand allows a business or service to prepare and increase supply as needed.</p><p><br></p><p><br></p><h2>Load and understand the data</h2><p><br></p><p>We begin by loading our data, which is stored in Comma-Separated Value (CSV) format. For that, we use the CSV node from SparkFlows, which creates a Spark DataFrame containing the dataset.</p><p><br></p><p><br></p><h2>Data description</h2><p><br></p><ul><li>From the UCI ML Repository description, we know that the columns have the following meanings.</li><li>Feature columns:</li><li>datetime : timestamp</li><li>season: season (1:spring, 2:summer, 3:fall, 4:winter)</li><li>holiday: whether day is holiday or not</li><li>workingday: if day is neither weekend nor holiday is 1, otherwise is 0.</li></ul><p><br></p><ul><li>temp: Normalized temperature in Celsius. The values are derived via (t-t_min)/(t_max-t_min), t_min=-8, t_max=+39 (only in hourly scale)</li><li>atemp: Normalized feeling temperature in Celsius. The values are derived via (t-t_min)/(t_max-t_min), t_min=-16, t_max=+50 (only in hourly scale)</li><li>humidity: Normalized humidity. The values are divided to 100 (max)</li><li>windspeed: Normalized wind speed. The values are divided to 67 (max)</li></ul><p><br></p><p><br></p><h2>Label columns:</h2><p><br></p><ul><li>casual: count of casual users</li><li>registered: count of registered users</li><li>cnt: count of total rental bikes including both casual and registered</li></ul><p><br></p><p><br></p><h2>Workflow</h2><p><br></p><ul><li>Preprocess the data</li><li>Extract year, month, day of month and hour from datetime field useing DateTimeFieldExtract Node.</li><li>Cast the cunt field type to double.</li></ul><p><br></p><h3><strong>Feature assembler and indexer</strong></h3><p><br></p><ul><li>Assemble feature columns into a feature vector.</li><li>Identify categorical features, and index them.</li></ul><p><br></p><h3><strong>Split data into training and test sets</strong></h3><p><br></p><ul><li>Our final data preparation step will split our dataset into separate training and test sets. We can train and tune our model as much as we like on the training set, as long as we do not look at the test set. After we have a good model (based on the training set), we can validate it on the held-out test set in order to know with high confidence our well our model will make predictions on future (unseen) data.</li></ul><p><br></p><h3><strong>Visualize our data</strong></h3><p><br></p><ul><li>Now that we have preprocessed our features and prepared a training dataset, we can quickly visualize our data to get a sense of whether the features are meaningful.</li><li>In the plot, we compare bike rental counts versus hour of the day. As one might expect, rentals are low during the night, and they peak in the morning (8am) and in the early evening (6pm).</li></ul>",					"widget": "textarea_rich",					"title": "Comment",					"description": "Comments for the Workflow",					"required": false,					"display": true,					"editable": true,					"disableRefresh": false				}			],			"engine": "all"		}	],	"edges": [		{			"source": "1",			"target": "2",			"id": 1		},		{			"source": "2",			"target": "3",			"id": 2		},		{			"source": "3",			"target": "4",			"id": 3		},		{			"source": "4",			"target": "5",			"id": 4		},		{			"source": "5",			"target": "6",			"id": 5		},		{			"source": "6",			"target": "7",			"id": 6		},		{			"source": "7",			"target": "8",			"id": 7		},		{			"source": "6",			"target": "8",			"id": 8		},		{			"source": "8",			"target": "9",			"id": 9		},		{			"source": "2",			"target": "10",			"id": 10		},		{			"source": "10",			"target": "11",			"id": 11		},		{			"source": "1",			"target": "13",			"id": 12		},		{			"source": "13",			"target": "14",			"id": 13		}	],	"dataSetDetails": [		{			"id": 642,			"uuid": "d41de163-42b1-4d60-b7df-2d45eac03710",			"header": true,			"path": "data/bike_sharing_sample_dataset.csv",			"delimiter": ",",			"datasetType": "CSV",			"filterLinesContaining": "season",			"datasetSchema": "{\"colNames\":[\"datetime\",\"season\",\"holiday\",\"workingday\",\"weather\",\"temp\",\"atemp\",\"humidity\",\"windspeed\",\"casual\",\"registered\",\"count\"],\"colTypes\":[\"TIMESTAMP\",\"INTEGER\",\"INTEGER\",\"INTEGER\",\"INTEGER\",\"DOUBLE\",\"DOUBLE\",\"INTEGER\",\"DOUBLE\",\"INTEGER\",\"INTEGER\",\"INTEGER\"],\"colFormats\":[\"dd/MM/yyyy HH:mm\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\"],\"colMLTypes\":[\"TEXT\",\"NUMERIC\",\"NUMERIC\",\"NUMERIC\",\"NUMERIC\",\"NUMERIC\",\"NUMERIC\",\"NUMERIC\",\"NUMERIC\",\"NUMERIC\",\"NUMERIC\",\"NUMERIC\"]}"		}	],	"engine": "scala"}