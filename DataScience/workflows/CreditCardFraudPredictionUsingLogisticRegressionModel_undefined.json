{	"name": "Credit Card Fraud Prediction Using Logistic Regression Model",	"uuid": "880eb963-7eb3-4a44-9004-8f9dcaefe764",	"category": "Prediction",	"description": "Predict if the customer will default or not",	"parameters": "-",	"nodes": [		{			"id": "1",			"name": "Read credit card data",			"description": "It reads in CSV files and creates a DataFrame from it",			"details": "This node reads CSV files and creates a DataFrame from it.<br>",			"examples": "",			"type": "dataset",			"nodeClass": "fire.nodes.dataset.NodeDatasetCSV",			"x": "46.7344px",			"y": "120.391px",			"hint": "Whenever the file is changed, Refresh the Schema",			"fields": [				{					"name": "storageLevel",					"value": "DEFAULT",					"widget": "array",					"title": "Output Storage Level",					"description": "Storage Level of the Output Datasets of this Node",					"optionsArray": [						"DEFAULT",						"NONE",						"DISK_ONLY",						"DISK_ONLY_2",						"MEMORY_ONLY",						"MEMORY_ONLY_2",						"MEMORY_ONLY_SER",						"MEMORY_ONLY_SER_2",						"MEMORY_AND_DISK",						"MEMORY_AND_DISK_2",						"MEMORY_AND_DISK_SER",						"MEMORY_AND_DISK_SER_2",						"OFF_HEAP"					],					"required": false,					"display": true,					"editable": true,					"disableRefresh": false				},				{					"name": "path",					"value": "data/UCI_Credit_Card.csv",					"widget": "textfield",					"title": "Path",					"description": "Path of the Text file/directory",					"required": true,					"display": true,					"editable": true,					"disableRefresh": false				},				{					"name": "separator",					"value": ",",					"widget": "textfield",					"title": "Separator",					"description": "CSV Separator",					"required": false,					"display": true,					"editable": true,					"disableRefresh": false				},				{					"name": "header",					"value": "true",					"widget": "array",					"title": "Header",					"description": "Does the file have a header row",					"optionsArray": [						"true",						"false"					],					"required": false,					"display": true,					"editable": true,					"disableRefresh": false				},				{					"name": "dropSpecialCharacterInColumnName",					"value": "true",					"widget": "array",					"title": "Drop Special Character In ColumnName",					"description": "Drop the SpecialCharacter and Space in Column Name.",					"optionsArray": [						"true",						"false"					],					"required": false,					"display": true,					"editable": true,					"disableRefresh": false				},				{					"name": "dropMalformed",					"value": "true",					"widget": "array",					"title": "Drop Malformed",					"description": "Whether to drop Malformed records or error",					"optionsArray": [						"true",						"false"					],					"required": false,					"display": true,					"editable": true,					"disableRefresh": false				},				{					"name": "outputColNames",					"value": "[\"ID\",\"LIMIT_BAL\",\"SEX\",\"EDUCATION\",\"MARRIAGE\",\"AGE\",\"PAY_0\",\"PAY_2\",\"PAY_3\",\"PAY_4\",\"PAY_5\",\"PAY_6\",\"BILL_AMT1\",\"BILL_AMT2\",\"BILL_AMT3\",\"BILL_AMT4\",\"BILL_AMT5\",\"BILL_AMT6\",\"PAY_AMT1\",\"PAY_AMT2\",\"PAY_AMT3\",\"PAY_AMT4\",\"PAY_AMT5\",\"PAY_AMT6\",\"default_payment_next_month\"]",					"widget": "schema_col_names",					"title": "Column Names for the CSV",					"description": "New Output Columns of the SQL",					"required": false,					"display": true,					"editable": true,					"disableRefresh": false				},				{					"name": "outputColTypes",					"value": "[\"INTEGER\",\"DOUBLE\",\"INTEGER\",\"INTEGER\",\"INTEGER\",\"INTEGER\",\"INTEGER\",\"INTEGER\",\"INTEGER\",\"INTEGER\",\"INTEGER\",\"INTEGER\",\"INTEGER\",\"INTEGER\",\"INTEGER\",\"INTEGER\",\"INTEGER\",\"INTEGER\",\"INTEGER\",\"INTEGER\",\"INTEGER\",\"INTEGER\",\"INTEGER\",\"INTEGER\",\"DOUBLE\"]",					"widget": "schema_col_types",					"title": "Column Types for the CSV",					"description": "Data Type of the Output Columns",					"required": false,					"display": true,					"editable": true,					"disableRefresh": false				},				{					"name": "outputColFormats",					"value": "[\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\"]",					"widget": "schema_col_formats",					"title": "Column Formats for the CSV",					"description": "Format of the Output Columns",					"required": false,					"display": true,					"editable": true,					"disableRefresh": false				}			],			"engine": "scala"		},		{			"id": "4",			"name": "VectorAssembler",			"description": "Merges multiple columns into a vector column",			"details": "VectorAssembler is a transformer that combines a given list of columns into a single vector column. <br>\nIt is useful for combining raw features and features generated by different feature transformers into a single feature vector, in order to train ML models like logistic regression and decision trees. <br>\nVectorAssembler accepts the following input column types: all numeric types, boolean type, and vector type. In each row, the values of the input columns will be concatenated into a vector in the specified order.<br>\n<br>\nMore details are available at:<br>\n<br>\n<a href=\"https://spark.apache.org/docs/latest/ml-features.html#vectorassembler\" target=\"_blank\">spark.apache.org/docs/latest/ml-features.html#vectorassembler</a><br>",			"examples": "<h2>The below example is available at : <a href=\"https://spark.apache.org/docs/latest/ml-features.html#vectorassembler\" target=\"_blank\">spark.apache.org/docs/latest/ml-features.html#vectorassembler</a></h2>\n<br>\nimport org.apache.spark.ml.feature.VectorAssembler<br>\nimport org.apache.spark.ml.linalg.Vectors<br>\n<br>\nval dataset = spark.createDataFrame(<br>\n  Seq((0, 18, 1.0, Vectors.dense(0.0, 10.0, 0.5), 1.0))<br>\n).toDF(\"id\", \"hour\", \"mobile\", \"userFeatures\", \"clicked\")<br>\n<br>\nval assembler = new VectorAssembler()<br>\n  .setInputCols(Array(\"hour\", \"mobile\", \"userFeatures\"))<br>\n  .setOutputCol(\"features\")<br>\n<br>\nval output = assembler.transform(dataset)<br>\nprintln(\"Assembled columns 'hour', 'mobile', 'userFeatures' to vector column 'features'\")<br>\noutput.select(\"features\", \"clicked\").show(false)<br>",			"type": "ml-transformer",			"nodeClass": "fire.nodes.ml.NodeVectorAssembler",			"x": "255.75px",			"y": "322.391px",			"fields": [				{					"name": "storageLevel",					"value": "DEFAULT",					"widget": "array",					"title": "Output Storage Level",					"description": "Storage Level of the Output Datasets of this Node",					"optionsArray": [						"DEFAULT",						"NONE",						"DISK_ONLY",						"DISK_ONLY_2",						"MEMORY_ONLY",						"MEMORY_ONLY_2",						"MEMORY_ONLY_SER",						"MEMORY_ONLY_SER_2",						"MEMORY_AND_DISK",						"MEMORY_AND_DISK_2",						"MEMORY_AND_DISK_SER",						"MEMORY_AND_DISK_SER_2",						"OFF_HEAP"					],					"required": false,					"display": true,					"editable": true,					"disableRefresh": false				},				{					"name": "inputCols",					"value": "[\"LIMIT_BAL\",\"SEX\",\"EDUCATION\",\"MARRIAGE\",\"AGE\",\"PAY_0\",\"PAY_2\",\"PAY_3\",\"PAY_4\",\"PAY_5\",\"PAY_6\",\"BILL_AMT1\",\"BILL_AMT2\",\"BILL_AMT3\",\"BILL_AMT4\",\"BILL_AMT5\",\"BILL_AMT6\",\"PAY_AMT1\",\"PAY_AMT2\",\"PAY_AMT3\",\"PAY_AMT4\",\"PAY_AMT5\",\"PAY_AMT6\",\"sex_one_hot\",\"education_one_hot\",\"marriage_one_hot\"]",					"widget": "variables",					"title": "Input Columns",					"description": "Input column of type - all numeric, boolean and vector",					"datatypes": [						"integer",						"long",						"double",						"float",						"vectorudt"					],					"required": false,					"display": true,					"editable": true,					"disableRefresh": false				},				{					"name": "outputCol",					"value": "feature_vector",					"widget": "textfield",					"title": "Output Column",					"description": "Output column name",					"required": true,					"display": true,					"editable": true,					"disableRefresh": false				},				{					"name": "handleInvalid",					"value": "error",					"widget": "array",					"title": "HandleInvalid",					"description": "How to handle invalid data (NULL values). Options are 'skip' (filter out rows with invalid data), 'error' (throw an error), or 'keep' (return relevant number of NaN in the output).",					"optionsArray": [						"error",						"skip",						"keep"					],					"required": false,					"display": true,					"editable": true,					"disableRefresh": false				}			],			"engine": "scala"		},		{			"id": "6",			"name": "Split into 80/20",			"description": "This node splits the incoming DataFrame into 2. It takes in the fraction to use in splitting the data. For example, if the fraction is .7, it would split the data into 2 DataFrames, one containing 70% of the rows(passed from lower edge id to next node) and the other containing the remaining 30%(passed from higher edge id to next node).",			"details": "This node splits the incoming DataFrame into 2. It takes in the fraction to use in splitting the data.<br>\n<br>\nFor example, if the fraction is .7, it would split the data into 2 DataFrames, one containing 70% of the rows(passed from lower edge id to next node) and the other containing the remaining 30%(passed from higher edge id to next node).<br>\n<br>\nThe split node can be used for splitting the DataFrame for training and test datasets used in Machine Learning.<br>",			"examples": "",			"type": "transform",			"nodeClass": "fire.nodes.ml.NodeSplit",			"x": "468.719px",			"y": "323.375px",			"fields": [				{					"name": "storageLevel",					"value": "DEFAULT",					"widget": "array",					"title": "Output Storage Level",					"description": "Storage Level of the Output Datasets of this Node",					"optionsArray": [						"DEFAULT",						"NONE",						"DISK_ONLY",						"DISK_ONLY_2",						"MEMORY_ONLY",						"MEMORY_ONLY_2",						"MEMORY_ONLY_SER",						"MEMORY_ONLY_SER_2",						"MEMORY_AND_DISK",						"MEMORY_AND_DISK_2",						"MEMORY_AND_DISK_SER",						"MEMORY_AND_DISK_SER_2",						"OFF_HEAP"					],					"required": false,					"display": true,					"editable": true,					"disableRefresh": false				},				{					"name": "fraction1",					"value": ".8",					"widget": "textfield",					"title": "Fraction 1",					"description": "Fraction to be used for Splitting the DataFrame into two. The first DataFrame would go to the lower edge output. The other would go to the higher edge output.",					"required": true,					"display": true,					"editable": true,					"disableRefresh": false				}			],			"engine": "scala"		},		{			"id": "7",			"name": "Predict",			"description": "Predict node takes in a DataFrame and Model and makes predictions",			"details": "Predict node takes in a DataFrame and Model and makes predictions on the data using the Model.<br>",			"examples": "",			"type": "ml-predict",			"nodeClass": "fire.nodes.ml.NodePredict",			"x": "765.734px",			"y": "452.375px",			"fields": [				{					"name": "storageLevel",					"value": "DEFAULT",					"widget": "array",					"title": "Output Storage Level",					"description": "Storage Level of the Output Datasets of this Node",					"optionsArray": [						"DEFAULT",						"NONE",						"DISK_ONLY",						"DISK_ONLY_2",						"MEMORY_ONLY",						"MEMORY_ONLY_2",						"MEMORY_ONLY_SER",						"MEMORY_ONLY_SER_2",						"MEMORY_AND_DISK",						"MEMORY_AND_DISK_2",						"MEMORY_AND_DISK_SER",						"MEMORY_AND_DISK_SER_2",						"OFF_HEAP"					],					"required": false,					"display": true,					"editable": true,					"disableRefresh": false				}			],			"engine": "scala"		},		{			"id": "8",			"name": "BinaryClassificationEvaluator",			"description": "Evaluator for binary classification, which expects two input columns: rawPrediction and label.",			"details": "Evaluator for binary classification, which expects two input columns: rawPrediction and label.<br>\n<br>\n<br>\nMore at Spark MLlib/ML docs page : <a href=\"http://spark.apache.org/docs/latest/mllib-evaluation-metrics.html#binary-classification\" target=\"_blank\">spark.apache.org/docs/latest/mllib-evaluation-metrics.html#binary-classification</a><br>",			"examples": "<h2>Below example is available at : <a href=\"https://spark.apache.org/docs/latest/mllib-evaluation-metrics.html#binary-classification\" target=\"_blank\">spark.apache.org/docs/latest/mllib-evaluation-metrics.html#binary-classification</a></h2>\n<br>\nimport org.apache.spark.mllib.classification.LogisticRegressionWithLBFGS<br>\nimport org.apache.spark.mllib.evaluation.BinaryClassificationMetrics<br>\nimport org.apache.spark.mllib.regression.LabeledPoint<br>\nimport org.apache.spark.mllib.util.MLUtils<br>\n<br>\n// Load training data in LIBSVM format<br>\nval data = MLUtils.loadLibSVMFile(sc, \"data/mllib/sample_binary_classification_data.txt\")<br>\n<br>\n// Split data into training (60%) and test (40%)<br>\nval Array(training, test) = data.randomSplit(Array(0.6, 0.4), seed = 11L)<br>\ntraining.cache()<br>\n<br>\n// Run training algorithm to build the model<br>\nval model = new LogisticRegressionWithLBFGS()<br>\n  .setNumClasses(2)<br>\n  .run(training)<br>\n<br>\n// Clear the prediction threshold so the model will return probabilities<br>\nmodel.clearThreshold<br>\n<br>\n// Compute raw scores on the test set<br>\nval predictionAndLabels = test.map { case LabeledPoint(label, features) =><br>\n  val prediction = model.predict(features)<br>\n  (prediction, label)<br>\n}<br>\n<br>\n// Instantiate metrics object<br>\nval metrics = new BinaryClassificationMetrics(predictionAndLabels)<br>\n<br>\n// Precision by threshold<br>\nval precision = metrics.precisionByThreshold<br>\nprecision.collect.foreach { case (t, p) =><br>\n  println(s\"Threshold: $t, Precision: $p\")<br>\n}<br>\n<br>\n// Recall by threshold<br>\nval recall = metrics.recallByThreshold<br>\nrecall.collect.foreach { case (t, r) =><br>\n  println(s\"Threshold: $t, Recall: $r\")<br>\n}<br>\n<br>\n// Precision-Recall Curve<br>\nval PRC = metrics.pr<br>\n<br>\n// F-measure<br>\nval f1Score = metrics.fMeasureByThreshold<br>\nf1Score.collect.foreach { case (t, f) =><br>\n  println(s\"Threshold: $t, F-score: $f, Beta = 1\")<br>\n}<br>\n<br>\nval beta = 0.5<br>\nval fScore = metrics.fMeasureByThreshold(beta)<br>\nfScore.collect.foreach { case (t, f) =><br>\n  println(s\"Threshold: $t, F-score: $f, Beta = 0.5\")<br>\n}<br>\n<br>\n// AUPRC<br>\nval auPRC = metrics.areaUnderPR<br>\nprintln(s\"Area under precision-recall curve = $auPRC\")<br>\n<br>\n// Compute thresholds used in ROC and PR curves<br>\nval thresholds = precision.map(_._1)<br>\n<br>\n// ROC Curve<br>\nval roc = metrics.roc<br>\n<br>\n// AUROC<br>\nval auROC = metrics.areaUnderROC<br>\nprintln(s\"Area under ROC = $auROC\")<br>",			"type": "ml-evaluator",			"nodeClass": "fire.nodes.ml.NodeBinaryClassificationEvaluator",			"x": "999.469px",			"y": "450.109px",			"fields": [				{					"name": "storageLevel",					"value": "DEFAULT",					"widget": "array",					"title": "Output Storage Level",					"description": "Storage Level of the Output Datasets of this Node",					"optionsArray": [						"DEFAULT",						"NONE",						"DISK_ONLY",						"DISK_ONLY_2",						"MEMORY_ONLY",						"MEMORY_ONLY_2",						"MEMORY_ONLY_SER",						"MEMORY_ONLY_SER_2",						"MEMORY_AND_DISK",						"MEMORY_AND_DISK_2",						"MEMORY_AND_DISK_SER",						"MEMORY_AND_DISK_SER_2",						"OFF_HEAP"					],					"required": false,					"display": true,					"editable": true,					"disableRefresh": false				},				{					"name": "labelCol",					"value": "default_payment_next_month",					"widget": "variable",					"title": "Label Column",					"description": "The label column for model fitting.",					"datatypes": [						"double"					],					"required": true,					"display": true,					"editable": true,					"disableRefresh": false				},				{					"name": "predictionCol",					"value": "prediction",					"widget": "variable",					"title": "Prediction Column",					"description": "The prediction column.",					"datatypes": [						"double"					],					"required": false,					"display": true,					"editable": true,					"disableRefresh": false				},				{					"name": "metricName",					"value": "areaUnderROC",					"widget": "array",					"title": "Metric Name",					"description": "The metric used in evaluation.",					"optionsArray": [						"areaUnderROC",						"areaUnderPR",						"gini"					],					"required": false,					"display": true,					"editable": true,					"disableRefresh": false				}			],			"engine": "scala"		},		{			"id": "10",			"name": "LogisticRegression",			"description": "Logistic regression. Currently, this class only supports binary classification.",			"details": "Logistic regression is a popular method to predict a categorical response. <br>\n<br>\nIt is a special case of Generalized Linear models that predicts the probability of the outcomes. <br>\nIn spark.ml logistic regression can be used to predict a binary outcome by using binomial logistic regression, or it can be used to predict a multiclass outcome by using multinomial logistic regression.<br>\n<br>\nMore details are available at:<br>\n<br>\n<a href=\"http://spark.apache.org/docs/latest/ml-classification-regression.html#logistic-regression\" target=\"_blank\">spark.apache.org/docs/latest/ml-classification-regression.html#logistic-regression</a><br>",			"examples": "<h2>The below example is available at : <a href=\"https://spark.apache.org/docs/2.3.0/ml-classification-regression.html#logistic-regression\" target=\"_blank\">spark.apache.org/docs/2.3.0/ml-classification-regression.html#logistic-regression</a></h2>\n<br>\n<br>\nimport org.apache.spark.ml.classification.LogisticRegression<br>\n<br>\n// Load training data<br>\nval training = spark.read.format(\"libsvm\").load(\"data/mllib/sample_libsvm_data.txt\")<br>\n<br>\nval lr = new LogisticRegression()<br>\n  .setMaxIter(10)<br>\n  .setRegParam(0.3)<br>\n  .setElasticNetParam(0.8)<br>\n<br>\n// Fit the model<br>\nval lrModel = lr.fit(training)<br>\n<br>\n// Print the coefficients and intercept for logistic regression<br>\nprintln(s\"Coefficients: ${lrModel.coefficients} Intercept: ${lrModel.intercept}\")<br>\n<br>\n// We can also use the multinomial family for binary classification<br>\nval mlr = new LogisticRegression()<br>\n  .setMaxIter(10)<br>\n  .setRegParam(0.3)<br>\n  .setElasticNetParam(0.8)<br>\n  .setFamily(\"multinomial\")<br>\n<br>\nval mlrModel = mlr.fit(training)<br>\n<br>\n// Print the coefficients and intercepts for logistic regression with multinomial family<br>\nprintln(s\"Multinomial coefficients: ${mlrModel.coefficientMatrix}\")<br>\nprintln(s\"Multinomial intercepts: ${mlrModel.interceptVector}\")<br>",			"type": "ml-estimator",			"nodeClass": "fire.nodes.ml.NodeLogisticRegression",			"x": "765.484px",			"y": "137.125px",			"fields": [				{					"name": "storageLevel",					"value": "DEFAULT",					"widget": "array",					"title": "Output Storage Level",					"description": "Storage Level of the Output Datasets of this Node",					"optionsArray": [						"DEFAULT",						"NONE",						"DISK_ONLY",						"DISK_ONLY_2",						"MEMORY_ONLY",						"MEMORY_ONLY_2",						"MEMORY_ONLY_SER",						"MEMORY_ONLY_SER_2",						"MEMORY_AND_DISK",						"MEMORY_AND_DISK_2",						"MEMORY_AND_DISK_SER",						"MEMORY_AND_DISK_SER_2",						"OFF_HEAP"					],					"required": false,					"display": true,					"editable": true,					"disableRefresh": false				},				{					"name": "featuresCol",					"value": "feature_vector",					"widget": "variable",					"title": "Features Column",					"description": "Features column of type vectorUDT for model fitting",					"datatypes": [						"vectorudt"					],					"required": true,					"display": true,					"editable": true,					"disableRefresh": false				},				{					"name": "labelCol",					"value": "default_payment_next_month",					"widget": "variable",					"title": "Label Column",					"description": "The label column for model fitting",					"datatypes": [						"double"					],					"required": true,					"display": true,					"editable": true,					"disableRefresh": false				},				{					"name": "predictionCol",					"value": "",					"widget": "textfield",					"title": "Prediction Column",					"description": "The prediction column created during model scoring",					"required": false,					"display": true,					"editable": true,					"disableRefresh": false				},				{					"name": "maxIter",					"value": "100",					"widget": "textfield",					"title": "Maximum Iterations",					"description": "Maximum number of iterations (>= 0)",					"datatypes": [						"integer"					],					"required": false,					"display": true,					"editable": true,					"disableRefresh": false				},				{					"name": "fitIntercept",					"value": "true",					"widget": "array",					"title": "Fit Intercept",					"description": "Whether to fit an intercept term",					"datatypes": [						"boolean"					],					"optionsArray": [						"true",						"false"					],					"required": false,					"display": true,					"editable": true,					"disableRefresh": false				},				{					"name": "regParam",					"value": "0.0",					"widget": "textfield",					"title": "Regularization Param",					"description": "The regularization parameter",					"datatypes": [						"double"					],					"required": false,					"display": true,					"editable": true,					"disableRefresh": false				},				{					"name": "standardization",					"value": "true",					"widget": "array",					"title": "Standardization",					"description": "Whether to standardize the training features before fitting the model",					"datatypes": [						"boolean"					],					"optionsArray": [						"true",						"false"					],					"required": false,					"display": true,					"editable": true,					"disableRefresh": false				},				{					"name": "threshold",					"value": "0.5",					"widget": "textfield",					"title": "Threshold",					"description": "The threshold in binary classification prediction",					"datatypes": [						"double"					],					"required": false,					"display": true,					"editable": true,					"disableRefresh": false				},				{					"name": "tol",					"value": "1E-6",					"widget": "textfield",					"title": "Tolerance",					"description": "The convergence tolerance for iterative algorithms",					"datatypes": [						"double"					],					"required": false,					"display": true,					"editable": true,					"disableRefresh": false				},				{					"name": "elasticNetParam",					"value": "0.0",					"widget": "textfield",					"title": "ElasticNet Param",					"description": "The ElasticNet mixing parameter. For alpha = 0, the penalty is an L2 penalty. For alpha = 1, it is an L1 penalty",					"datatypes": [						"double"					],					"required": false,					"display": true,					"editable": true,					"disableRefresh": false				},				{					"name": "weightCol",					"value": "",					"widget": "textfield",					"title": "Weight Column",					"description": "If the 'weight column' is not specified, all instances are treated equally with a weight 1.0",					"required": false,					"display": true,					"editable": true,					"disableRefresh": false				},				{					"name": "gridSearch",					"value": "",					"widget": "tab",					"title": "Grid Search",					"required": false,					"display": true,					"editable": true,					"disableRefresh": false				},				{					"name": "regParamGrid",					"value": "",					"widget": "textfield",					"title": "Regularization Param Grid Search",					"description": "Regularization Parameters for Grid Search",					"required": false,					"display": true,					"editable": true,					"disableRefresh": false				},				{					"name": "elasticNetGrid",					"value": "",					"widget": "textfield",					"title": "ElasticNet Param Grid Search",					"description": "ElasticNet Parameters for Grid Search",					"required": false,					"display": true,					"editable": true,					"disableRefresh": false				}			],			"engine": "scala"		},		{			"id": "12",			"name": "OneHotEncoder",			"description": "Maps a column of label indices to a column of binary vectors, with at most a single one-value",			"details": "One-hot encoding maps a column of label indices to a column of binary vectors, with at most a single one-value. <br>\nThis encoding allows algorithms which expect continuous features, such as Logistic Regression, to use categorical features.<br>\n<br>\nMore at Spark MLlib/ML docs page : <a href=\"https://spark.apache.org/docs/2.0.0/ml-features.html#onehotencoder\" target=\"_blank\">spark.apache.org/docs/2.0.0/ml-features.html#onehotencoder</a><br>",			"examples": "<h2>The below example is available at : <a href=\"https://spark.apache.org/docs/2.0.0/ml-features.html#onehotencoder\" target=\"_blank\">spark.apache.org/docs/2.0.0/ml-features.html#onehotencoder</a></h2>\n<br>\nimport org.apache.spark.ml.feature.{OneHotEncoder, StringIndexer}<br>\n<br>\nval df = spark.createDataFrame(Seq(<br>\n  (0, \"a\"),<br>\n  (1, \"b\"),<br>\n  (2, \"c\"),<br>\n  (3, \"a\"),<br>\n  (4, \"a\"),<br>\n  (5, \"c\")<br>\n)).toDF(\"id\", \"category\")<br>\n<br>\nval indexer = new StringIndexer()<br>\n  .setInputCol(\"category\")<br>\n  .setOutputCol(\"categoryIndex\")<br>\n  .fit(df)<br>\nval indexed = indexer.transform(df)<br>\n<br>\nval encoder = new OneHotEncoder()<br>\n  .setInputCol(\"categoryIndex\")<br>\n  .setOutputCol(\"categoryVec\")<br>\nval encoded = encoder.transform(indexed)<br>\nencoded.select(\"id\", \"categoryVec\").show()<br>",			"type": "ml-transformer",			"nodeClass": "fire.nodes.ml.NodeOneHotEncoder",			"x": "252.719px",			"y": "120.375px",			"fields": [				{					"name": "storageLevel",					"value": "DEFAULT",					"widget": "array",					"title": "Output Storage Level",					"description": "Storage Level of the Output Datasets of this Node",					"optionsArray": [						"DEFAULT",						"NONE",						"DISK_ONLY",						"DISK_ONLY_2",						"MEMORY_ONLY",						"MEMORY_ONLY_2",						"MEMORY_ONLY_SER",						"MEMORY_ONLY_SER_2",						"MEMORY_AND_DISK",						"MEMORY_AND_DISK_2",						"MEMORY_AND_DISK_SER",						"MEMORY_AND_DISK_SER_2",						"OFF_HEAP"					],					"required": false,					"display": true,					"editable": true,					"disableRefresh": false				},				{					"name": "inputCols",					"value": "[\"SEX\",\"EDUCATION\",\"MARRIAGE\"]",					"widget": "variables_list_select",					"title": "Input Columns",					"description": "Input columns for encoding",					"datatypes": [						"integer",						"long",						"double",						"float"					],					"required": false,					"display": true,					"editable": true,					"disableRefresh": false				},				{					"name": "outputCols",					"value": "[\"sex_one_hot\",\"education_one_hot\",\"marriage_one_hot\"]",					"widget": "variables_list_textfield",					"title": "Output Columns",					"description": "Output columns",					"required": false,					"display": true,					"editable": true,					"disableRefresh": false				}			],			"engine": "scala"		},		{			"id": "13",			"name": "Sticky Note",			"description": "Allows capturing Notes on the Workflow",			"details": "",			"examples": "",			"type": "sticky",			"nodeClass": "fire.nodes.doc.NodeStickyNote",			"x": "331px",			"y": "119px",			"fields": [				{					"name": "storageLevel",					"value": "DEFAULT",					"widget": "array",					"title": "Output Storage Level",					"description": "Storage Level of the Output Datasets of this Node",					"optionsArray": [						"DEFAULT",						"NONE",						"DISK_ONLY",						"DISK_ONLY_2",						"MEMORY_ONLY",						"MEMORY_ONLY_2",						"MEMORY_ONLY_SER",						"MEMORY_ONLY_SER_2",						"MEMORY_AND_DISK",						"MEMORY_AND_DISK_2",						"MEMORY_AND_DISK_SER",						"MEMORY_AND_DISK_SER_2",						"OFF_HEAP"					],					"required": false,					"display": true,					"editable": true,					"disableRefresh": false				},				{					"name": "bgColor",					"value": "gray",					"widget": "textfield",					"title": "Bg Color",					"description": "Background of note",					"required": false,					"display": true,					"editable": true,					"disableRefresh": false				},				{					"name": "width",					"value": "225px",					"widget": "textfield",					"title": "Width",					"description": "Width of note",					"required": false,					"display": true,					"editable": true,					"disableRefresh": false				},				{					"name": "height",					"value": "74px",					"widget": "textfield",					"title": "Height",					"description": "Height of note",					"required": false,					"display": true,					"editable": true,					"disableRefresh": false				},				{					"name": "comment",					"value": "<p>Encode categorical features into numeric values for modelling</p>",					"widget": "textarea_rich",					"title": "Comment",					"description": "Comments for the Workflow",					"required": false,					"display": true,					"editable": true,					"disableRefresh": false				}			],			"engine": "all"		},		{			"id": "14",			"name": "Sticky Note",			"description": "Allows capturing Notes on the Workflow",			"details": "",			"examples": "",			"type": "sticky",			"nodeClass": "fire.nodes.doc.NodeStickyNote",			"x": "146px",			"y": "409px",			"fields": [				{					"name": "storageLevel",					"value": "DEFAULT",					"widget": "array",					"title": "Output Storage Level",					"description": "Storage Level of the Output Datasets of this Node",					"optionsArray": [						"DEFAULT",						"NONE",						"DISK_ONLY",						"DISK_ONLY_2",						"MEMORY_ONLY",						"MEMORY_ONLY_2",						"MEMORY_ONLY_SER",						"MEMORY_ONLY_SER_2",						"MEMORY_AND_DISK",						"MEMORY_AND_DISK_2",						"MEMORY_AND_DISK_SER",						"MEMORY_AND_DISK_SER_2",						"OFF_HEAP"					],					"required": false,					"display": true,					"editable": true,					"disableRefresh": false				},				{					"name": "bgColor",					"value": "gray",					"widget": "textfield",					"title": "Bg Color",					"description": "Background of note",					"required": false,					"display": true,					"editable": true,					"disableRefresh": false				},				{					"name": "width",					"value": "227px",					"widget": "textfield",					"title": "Width",					"description": "Width of note",					"required": false,					"display": true,					"editable": true,					"disableRefresh": false				},				{					"name": "height",					"value": "74px",					"widget": "textfield",					"title": "Height",					"description": "Height of note",					"required": false,					"display": true,					"editable": true,					"disableRefresh": false				},				{					"name": "comment",					"value": "<p>Assemble features into vector for modelling</p>",					"widget": "textarea_rich",					"title": "Comment",					"description": "Comments for the Workflow",					"required": false,					"display": true,					"editable": true,					"disableRefresh": false				}			],			"engine": "all"		},		{			"id": "15",			"name": "Sticky Note",			"description": "Allows capturing Notes on the Workflow",			"details": "",			"examples": "",			"type": "sticky",			"nodeClass": "fire.nodes.doc.NodeStickyNote",			"x": "197px",			"y": "8px",			"fields": [				{					"name": "storageLevel",					"value": "DEFAULT",					"widget": "array",					"title": "Output Storage Level",					"description": "Storage Level of the Output Datasets of this Node",					"optionsArray": [						"DEFAULT",						"NONE",						"DISK_ONLY",						"DISK_ONLY_2",						"MEMORY_ONLY",						"MEMORY_ONLY_2",						"MEMORY_ONLY_SER",						"MEMORY_ONLY_SER_2",						"MEMORY_AND_DISK",						"MEMORY_AND_DISK_2",						"MEMORY_AND_DISK_SER",						"MEMORY_AND_DISK_SER_2",						"OFF_HEAP"					],					"required": false,					"display": true,					"editable": true,					"disableRefresh": false				},				{					"name": "bgColor",					"value": "blue",					"widget": "textfield",					"title": "Bg Color",					"description": "Background of note",					"required": false,					"display": true,					"editable": true,					"disableRefresh": false				},				{					"name": "width",					"value": "557px",					"widget": "textfield",					"title": "Width",					"description": "Width of note",					"required": false,					"display": true,					"editable": true,					"disableRefresh": false				},				{					"name": "height",					"value": "58px",					"widget": "textfield",					"title": "Height",					"description": "Height of note",					"required": false,					"display": true,					"editable": true,					"disableRefresh": false				},				{					"name": "comment",					"value": "<p>Build logistic classification model using SparkML to predict payment defaults</p>",					"widget": "textarea_rich",					"title": "Comment",					"description": "Comments for the Workflow",					"required": false,					"display": true,					"editable": true,					"disableRefresh": false				}			],			"engine": "all"		}	],	"edges": [		{			"source": "6",			"target": "7",			"id": 1		},		{			"source": "7",			"target": "8",			"id": 2		},		{			"source": "6",			"target": "10",			"id": 3		},		{			"source": "10",			"target": "7",			"id": 4		},		{			"source": "12",			"target": "4",			"id": 5		},		{			"source": "4",			"target": "6",			"id": 6		},		{			"source": "1",			"target": "12",			"id": 7		}	],	"dataSetDetails": [],	"engine": "scala"}