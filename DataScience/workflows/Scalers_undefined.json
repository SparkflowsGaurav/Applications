{	"name": "Scalers",	"uuid": "3e10de91-16c4-4697-9d50-70d739f03cd0",	"category": "FeatureEngineering",	"nodes": [		{			"id": "6",			"name": "ReadCSV",			"description": "It reads in CSV files and creates a DataFrame from it",			"details": "This node reads CSV files and creates a DataFrame from it.<br>",			"examples": "",			"type": "dataset",			"nodeClass": "fire.nodes.dataset.NodeDatasetCSV",			"x": "69px",			"y": "305px",			"hint": "Whenever the file is changed, Refresh the Schema",			"fields": [				{					"name": "storageLevel",					"value": "DEFAULT",					"widget": "array",					"title": "Output Storage Level",					"description": "Storage Level of the Output Datasets of this Node",					"optionsArray": [						"DEFAULT",						"NONE",						"DISK_ONLY",						"DISK_ONLY_2",						"MEMORY_ONLY",						"MEMORY_ONLY_2",						"MEMORY_ONLY_SER",						"MEMORY_ONLY_SER_2",						"MEMORY_AND_DISK",						"MEMORY_AND_DISK_2",						"MEMORY_AND_DISK_SER",						"MEMORY_AND_DISK_SER_2",						"OFF_HEAP"					],					"required": false,					"display": true,					"editable": true,					"disableRefresh": false				},				{					"name": "path",					"value": "data/housing.csv",					"widget": "textfield",					"title": "Path",					"description": "Path of the Text file/directory",					"required": true,					"display": true,					"editable": true,					"disableRefresh": false				},				{					"name": "separator",					"value": ",",					"widget": "textfield",					"title": "Separator",					"description": "CSV Separator",					"required": false,					"display": true,					"editable": true,					"disableRefresh": false				},				{					"name": "header",					"value": "true",					"widget": "array",					"title": "Header",					"description": "Does the file have a header row",					"optionsArray": [						"true",						"false"					],					"required": false,					"display": true,					"editable": true,					"disableRefresh": false				},				{					"name": "dropSpecialCharacterInColumnName",					"value": "true",					"widget": "array",					"title": "Drop Special Character In ColumnName",					"description": "Drop the SpecialCharacter and Space in Column Name.",					"optionsArray": [						"true",						"false"					],					"required": false,					"display": true,					"editable": true,					"disableRefresh": false				},				{					"name": "dropMalformed",					"value": "true",					"widget": "array",					"title": "Drop Malformed",					"description": "Whether to drop Malformed records or error",					"optionsArray": [						"true",						"false"					],					"required": false,					"display": true,					"editable": true,					"disableRefresh": false				},				{					"name": "outputColNames",					"value": "[\"id\",\"price\",\"lotsize\",\"bedrooms\",\"bathrms\",\"stories\",\"driveway\",\"recroom\",\"fullbase\",\"gashw\",\"airco\",\"garagepl\",\"prefarea\"]",					"widget": "schema_col_names",					"title": "Column Names for the CSV",					"description": "New Output Columns of the SQL",					"required": false,					"display": true,					"editable": true,					"disableRefresh": false				},				{					"name": "outputColTypes",					"value": "[\"INTEGER\",\"DOUBLE\",\"INTEGER\",\"INTEGER\",\"INTEGER\",\"INTEGER\",\"STRING\",\"STRING\",\"STRING\",\"STRING\",\"STRING\",\"INTEGER\",\"STRING\"]",					"widget": "schema_col_types",					"title": "Column Types for the CSV",					"description": "Data Type of the Output Columns",					"required": false,					"display": true,					"editable": true,					"disableRefresh": false				},				{					"name": "outputColFormats",					"value": "[\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\"]",					"widget": "schema_col_formats",					"title": "Column Formats for the CSV",					"description": "Format of the Output Columns",					"required": false,					"display": true,					"editable": true,					"disableRefresh": false				}			],			"engine": "all"		},		{			"id": "8",			"name": "VectorAssembler",			"description": "Merges multiple columns into a vector column",			"details": "VectorAssembler is a transformer that combines a given list of columns into a single vector column. <br>\nIt is useful for combining raw features and features generated by different feature transformers into a single feature vector, in order to train ML models like logistic regression and decision trees. <br>\nVectorAssembler accepts the following input column types: all numeric types, boolean type, and vector type. In each row, the values of the input columns will be concatenated into a vector in the specified order.<br>\n<br>\nMore details are available at:<br>\n<br>\n<a href=\"https://spark.apache.org/docs/latest/ml-features.html#vectorassembler\" target=\"_blank\">spark.apache.org/docs/latest/ml-features.html#vectorassembler</a><br>",			"examples": "<h2>The below example is available at : <a href=\"https://spark.apache.org/docs/latest/ml-features.html#vectorassembler\" target=\"_blank\">spark.apache.org/docs/latest/ml-features.html#vectorassembler</a></h2>\n<br>\nimport org.apache.spark.ml.feature.VectorAssembler<br>\nimport org.apache.spark.ml.linalg.Vectors<br>\n<br>\nval dataset = spark.createDataFrame(<br>\n  Seq((0, 18, 1.0, Vectors.dense(0.0, 10.0, 0.5), 1.0))<br>\n).toDF(\"id\", \"hour\", \"mobile\", \"userFeatures\", \"clicked\")<br>\n<br>\nval assembler = new VectorAssembler()<br>\n  .setInputCols(Array(\"hour\", \"mobile\", \"userFeatures\"))<br>\n  .setOutputCol(\"features\")<br>\n<br>\nval output = assembler.transform(dataset)<br>\nprintln(\"Assembled columns 'hour', 'mobile', 'userFeatures' to vector column 'features'\")<br>\noutput.select(\"features\", \"clicked\").show(false)<br>",			"type": "ml-transformer",			"nodeClass": "fire.nodes.ml.NodeVectorAssembler",			"x": "314px",			"y": "308px",			"fields": [				{					"name": "storageLevel",					"value": "DEFAULT",					"widget": "array",					"title": "Output Storage Level",					"description": "Storage Level of the Output Datasets of this Node",					"optionsArray": [						"DEFAULT",						"NONE",						"DISK_ONLY",						"DISK_ONLY_2",						"MEMORY_ONLY",						"MEMORY_ONLY_2",						"MEMORY_ONLY_SER",						"MEMORY_ONLY_SER_2",						"MEMORY_AND_DISK",						"MEMORY_AND_DISK_2",						"MEMORY_AND_DISK_SER",						"MEMORY_AND_DISK_SER_2",						"OFF_HEAP"					],					"required": false,					"display": true,					"editable": true,					"disableRefresh": false				},				{					"name": "inputCols",					"value": "[\"lotsize\",\"bedrooms\",\"bathrms\",\"stories\",\"garagepl\"]",					"widget": "variables",					"title": "Input Columns",					"description": "Input column of type - all numeric, boolean and vector",					"datatypes": [						"integer",						"long",						"double",						"float",						"vectorudt"					],					"required": false,					"display": true,					"editable": true,					"disableRefresh": false				},				{					"name": "outputCol",					"value": "feature_v",					"widget": "textfield",					"title": "Output Column",					"description": "Output column name",					"required": true,					"display": true,					"editable": true,					"disableRefresh": false				},				{					"name": "handleInvalid",					"value": "error",					"widget": "array",					"title": "HandleInvalid",					"description": "How to handle invalid data (NULL values). Options are 'skip' (filter out rows with invalid data), 'error' (throw an error), or 'keep' (return relevant number of NaN in the output).",					"optionsArray": [						"error",						"skip",						"keep"					],					"required": false,					"display": true,					"editable": true,					"disableRefresh": false				}			],			"engine": "scala"		},		{			"id": "12",			"name": "PrintNRows",			"description": "Prints the specified number of records in the DataFrame. It is useful for seeing intermediate output",			"details": "This node is used to print incoming dataset.<br>\n<br>\nNumber of rows that needs to be printed can be configured in the node.<br>",			"examples": "",			"type": "transform",			"nodeClass": "fire.nodes.util.NodePrintFirstNRows",			"x": "792px",			"y": "192px",			"fields": [				{					"name": "storageLevel",					"value": "DEFAULT",					"widget": "array",					"title": "Output Storage Level",					"description": "Storage Level of the Output Datasets of this Node",					"optionsArray": [						"DEFAULT",						"NONE",						"DISK_ONLY",						"DISK_ONLY_2",						"MEMORY_ONLY",						"MEMORY_ONLY_2",						"MEMORY_ONLY_SER",						"MEMORY_ONLY_SER_2",						"MEMORY_AND_DISK",						"MEMORY_AND_DISK_2",						"MEMORY_AND_DISK_SER",						"MEMORY_AND_DISK_SER_2",						"OFF_HEAP"					],					"required": false,					"display": true,					"editable": true,					"disableRefresh": false				},				{					"name": "title",					"value": "Row Values",					"widget": "textfield",					"title": "Title",					"required": false,					"display": true,					"editable": true,					"disableRefresh": false				},				{					"name": "n",					"value": "10",					"widget": "textfield",					"title": "Num Rows to Print",					"description": "number of rows to be printed",					"required": false,					"display": true,					"editable": true,					"disableRefresh": false				},				{					"name": "displayDataType",					"value": "true",					"widget": "array",					"title": "Display Data Type",					"description": "If true display rows DataType",					"optionsArray": [						"true",						"false"					],					"required": false,					"display": true,					"editable": true,					"disableRefresh": false				}			],			"engine": "all"		},		{			"id": "15",			"name": "StickyNote",			"description": "Allows capturing Notes on the Workflow",			"details": "",			"examples": "",			"type": "sticky",			"nodeClass": "fire.nodes.doc.NodeStickyNote",			"x": "218px",			"y": "408px",			"fields": [				{					"name": "storageLevel",					"value": "DEFAULT",					"widget": "array",					"title": "Output Storage Level",					"description": "Storage Level of the Output Datasets of this Node",					"optionsArray": [						"DEFAULT",						"NONE",						"DISK_ONLY",						"DISK_ONLY_2",						"MEMORY_ONLY",						"MEMORY_ONLY_2",						"MEMORY_ONLY_SER",						"MEMORY_ONLY_SER_2",						"MEMORY_AND_DISK",						"MEMORY_AND_DISK_2",						"MEMORY_AND_DISK_SER",						"MEMORY_AND_DISK_SER_2",						"OFF_HEAP"					],					"required": false,					"display": true,					"editable": true,					"disableRefresh": false				},				{					"name": "bgColor",					"value": "gray",					"widget": "textfield",					"title": "Bg Color",					"description": "Background of note",					"required": false,					"display": true,					"editable": true,					"disableRefresh": false				},				{					"name": "width",					"value": "256px",					"widget": "textfield",					"title": "Width",					"description": "Width of note",					"required": false,					"display": true,					"editable": true,					"disableRefresh": false				},				{					"name": "height",					"value": "60px",					"widget": "textfield",					"title": "Height",					"description": "Height of note",					"required": false,					"display": true,					"editable": true,					"disableRefresh": false				},				{					"name": "comment",					"value": "<p>Assemble the features into a Vector.</p>",					"widget": "textarea_rich",					"title": "Comment",					"description": "Comments for the Workflow",					"required": false,					"display": true,					"editable": true,					"disableRefresh": false				}			],			"engine": "scala"		},		{			"id": "16",			"name": "Sticky Note",			"description": "Allows capturing Notes on the Workflow",			"details": "",			"examples": "",			"type": "sticky",			"nodeClass": "fire.nodes.doc.NodeStickyNote",			"x": "33px",			"y": "33px",			"fields": [				{					"name": "storageLevel",					"value": "DEFAULT",					"widget": "array",					"title": "Output Storage Level",					"description": "Storage Level of the Output Datasets of this Node",					"optionsArray": [						"DEFAULT",						"NONE",						"DISK_ONLY",						"DISK_ONLY_2",						"MEMORY_ONLY",						"MEMORY_ONLY_2",						"MEMORY_ONLY_SER",						"MEMORY_ONLY_SER_2",						"MEMORY_AND_DISK",						"MEMORY_AND_DISK_2",						"MEMORY_AND_DISK_SER",						"MEMORY_AND_DISK_SER_2",						"OFF_HEAP"					],					"required": false,					"display": true,					"editable": true,					"disableRefresh": false				},				{					"name": "bgColor",					"value": "blue",					"widget": "textfield",					"title": "Bg Color",					"description": "Background of note",					"required": false,					"display": true,					"editable": true,					"disableRefresh": false				},				{					"name": "width",					"value": "527px",					"widget": "textfield",					"title": "Width",					"description": "Width of note",					"required": false,					"display": true,					"editable": true,					"disableRefresh": false				},				{					"name": "height",					"value": "171px",					"widget": "textfield",					"title": "Height",					"description": "Height of note",					"required": false,					"display": true,					"editable": true,					"disableRefresh": false				},				{					"name": "comment",					"value": "<p><span style=\"color: rgb(74, 74, 74);\">Scaler transforms a dataset of Vector rows, </span></p><ol><li><span style=\"color: rgb(0, 0, 0);\">StandardScaler: normalizing each feature to have unit standard deviation and/or zero mean.</span></li><li class=\"ql-indent-1\"><span style=\"color: rgb(74, 74, 74);\">﻿https://spark.apache.org/docs/latest/ml-features#standardscaler</span></li><li><span style=\"color: rgb(0, 0, 0);\">MinMaxScaler: rescaling each feature to a specific range (often [0, 1]).</span></li><li class=\"ql-indent-1\">https://spark.apache.org/docs/latest/ml-features#minmaxscaler</li></ol><p><br></p>",					"widget": "textarea_rich",					"title": "Comment",					"description": "Comments for the Workflow",					"required": false,					"display": true,					"editable": true,					"disableRefresh": false				}			],			"engine": "all"		},		{			"id": "17",			"name": "Standard Scaler",			"description": "StandardScaler transforms a dataset of Vector rows, normalizing each feature to have unit standard deviation and/or zero mean.",			"details": "StandardScaler transforms a dataset of Vector rows, normalizing each feature to have unit standard deviation and/or zero mean. It takes parameters:<br>\n<br>\n<ul>\n<li> withStd: True by default. Scales the data to unit standard deviation.</li>\n<li> withMean: False by default. Centers the data with mean before scaling. It will build a dense output, so take care when applying to sparse input.</li>\n</ul>\nStandardScaler is an Estimator which can be fit on a dataset to produce a StandardScalerModel; this amounts to computing summary statistics. The model can then transform a Vector column in a dataset to have unit standard deviation and/or zero mean features.<br>\n<br>\nNote that if the standard deviation of a feature is zero, it will return default 0.0 value in the Vector for that feature.<br>\n<br>\nMore details are available at : <a href=\"http://spark.apache.org/docs/latest/ml-features.html#standardscaler\" target=\"_blank\">spark.apache.org/docs/latest/ml-features.html#standardscaler</a><br>",			"examples": "<h2>The below example is available at : <a href=\"https://spark.apache.org/docs/latest/ml-features.html#standardscaler\" target=\"_blank\">spark.apache.org/docs/latest/ml-features.html#standardscaler</a></h2>\n<br>\n<br>\nimport org.apache.spark.ml.feature.StandardScaler<br>\n<br>\nval dataFrame = spark.read.format(\"libsvm\").load(\"data/mllib/sample_libsvm_data.txt\")<br>\n<br>\nval scaler = new StandardScaler()<br>\n  .setInputCol(\"features\")<br>\n  .setOutputCol(\"scaledFeatures\")<br>\n  .setWithStd(true)<br>\n  .setWithMean(false)<br>\n<br>\n// Compute summary statistics by fitting the StandardScaler.<br>\nval scalerModel = scaler.fit(dataFrame)<br>\n<br>\n// Normalize each feature to have unit standard deviation.<br>\nval scaledData = scalerModel.transform(dataFrame)<br>\nscaledData.show()<br>",			"type": "ml-transformer",			"nodeClass": "fire.nodes.ml.NodeStandardScaler",			"x": "582px",			"y": "195px",			"fields": [				{					"name": "storageLevel",					"value": "DEFAULT",					"widget": "array",					"title": "Output Storage Level",					"description": "Storage Level of the Output Datasets of this Node",					"optionsArray": [						"DEFAULT",						"NONE",						"DISK_ONLY",						"DISK_ONLY_2",						"MEMORY_ONLY",						"MEMORY_ONLY_2",						"MEMORY_ONLY_SER",						"MEMORY_ONLY_SER_2",						"MEMORY_AND_DISK",						"MEMORY_AND_DISK_2",						"MEMORY_AND_DISK_SER",						"MEMORY_AND_DISK_SER_2",						"OFF_HEAP"					],					"required": false,					"display": true,					"editable": true,					"disableRefresh": false				},				{					"name": "inputCol",					"value": "feature_v",					"widget": "variable",					"title": "Input Column",					"description": "The input column name",					"datatypes": [						"vectorudt"					],					"required": true,					"display": true,					"editable": true,					"disableRefresh": false				},				{					"name": "outputCol",					"value": "scaled_feature_standard",					"widget": "textfield",					"title": "Output Column",					"description": "The output column name",					"datatypes": [						"vectorudt"					],					"required": true,					"display": true,					"editable": true,					"disableRefresh": false				},				{					"name": "withMean",					"value": "false",					"widget": "array",					"title": "With Mean",					"description": "Centers the data with mean before scaling.",					"datatypes": [						"boolean"					],					"optionsArray": [						"true",						"false"					],					"required": false,					"display": true,					"editable": true,					"disableRefresh": false				},				{					"name": "withStd",					"value": "true",					"widget": "array",					"title": "With Standard Dev",					"description": "Scales the data to unit standard deviation",					"datatypes": [						"boolean"					],					"optionsArray": [						"true",						"false"					],					"required": false,					"display": true,					"editable": true,					"disableRefresh": false				}			],			"engine": "all"		},		{			"id": "18",			"name": "Min Max Scaler",			"description": "MinMaxScaler transforms a dataset of Vector rows, rescaling each feature to a specific range (often [0, 1])",			"details": "MinMaxScaler transforms a dataset of Vector rows, rescaling each feature to a specific range (often [0, 1]). It takes parameters:<br>\n<br>\n<ul>\n<li> min: 0.0 by default. Lower bound after transformation, shared by all features.</li>\n<li> max: 1.0 by default. Upper bound after transformation, shared by all features.</li>\n</ul>\nMinMaxScaler computes summary statistics on a data set and produces a MinMaxScalerModel. The model can then transform each feature individually such that it is in the given range.<br>\n<br>\nMore details are available at : <a href=\"http://spark.apache.org/docs/latest/ml-features.html#minmaxscaler\" target=\"_blank\">spark.apache.org/docs/latest/ml-features.html#minmaxscaler</a><br>",			"examples": "<h2>The below example is available at : <a href=\"https://spark.apache.org/docs/latest/ml-features.html#minmaxscaler\" target=\"_blank\">spark.apache.org/docs/latest/ml-features.html#minmaxscaler</a></h2>\n<br>\n<br>\nimport org.apache.spark.ml.feature.MinMaxScaler<br>\nimport org.apache.spark.ml.linalg.Vectors<br>\n<br>\nval dataFrame = spark.createDataFrame(Seq(<br>\n  (0, Vectors.dense(1.0, 0.1, -1.0)),<br>\n  (1, Vectors.dense(2.0, 1.1, 1.0)),<br>\n  (2, Vectors.dense(3.0, 10.1, 3.0))<br>\n)).toDF(\"id\", \"features\")<br>\n<br>\nval scaler = new MinMaxScaler()<br>\n  .setInputCol(\"features\")<br>\n  .setOutputCol(\"scaledFeatures\")<br>\n<br>\n// Compute summary statistics and generate MinMaxScalerModel<br>\nval scalerModel = scaler.fit(dataFrame)<br>\n<br>\n// rescale each feature to range [min, max].<br>\nval scaledData = scalerModel.transform(dataFrame)<br>\nprintln(s\"Features scaled to range: [${scaler.getMin}, ${scaler.getMax}]\")<br>\nscaledData.select(\"features\", \"scaledFeatures\").show()<br>",			"type": "ml-transformer",			"nodeClass": "fire.nodes.ml.NodeMinMaxScaler",			"x": "584px",			"y": "403px",			"fields": [				{					"name": "storageLevel",					"value": "DEFAULT",					"widget": "array",					"title": "Output Storage Level",					"description": "Storage Level of the Output Datasets of this Node",					"optionsArray": [						"DEFAULT",						"NONE",						"DISK_ONLY",						"DISK_ONLY_2",						"MEMORY_ONLY",						"MEMORY_ONLY_2",						"MEMORY_ONLY_SER",						"MEMORY_ONLY_SER_2",						"MEMORY_AND_DISK",						"MEMORY_AND_DISK_2",						"MEMORY_AND_DISK_SER",						"MEMORY_AND_DISK_SER_2",						"OFF_HEAP"					],					"required": false,					"display": true,					"editable": true,					"disableRefresh": false				},				{					"name": "inputCol",					"value": "feature_v",					"widget": "variable",					"title": "Input Column ",					"description": "The input column name",					"datatypes": [						"vectorudt"					],					"required": true,					"display": true,					"editable": true,					"disableRefresh": false				},				{					"name": "outputCol",					"value": "scaled_feature_minmax",					"widget": "textfield",					"title": "Output Column",					"description": "The output column name",					"datatypes": [						"vectorudt"					],					"required": true,					"display": true,					"editable": true,					"disableRefresh": false				},				{					"name": "max",					"value": "1.0",					"widget": "textfield",					"title": "Max",					"description": "The upper bound after transformation, shared by all features",					"datatypes": [						"double"					],					"required": true,					"display": true,					"editable": true,					"disableRefresh": false				},				{					"name": "min",					"value": "0.0",					"widget": "textfield",					"title": "Min",					"description": "The lower bound after transformation, shared by all features",					"datatypes": [						"double"					],					"required": true,					"display": true,					"editable": true,					"disableRefresh": false				}			],			"engine": "scala"		},		{			"id": "19",			"name": "PrintNRows",			"description": "Prints the specified number of records in the DataFrame. It is useful for seeing intermediate output",			"details": "This node is used to print incoming dataset.<br>\n<br>\nNumber of rows that needs to be printed can be configured in the node.<br>",			"examples": "",			"type": "transform",			"nodeClass": "fire.nodes.util.NodePrintFirstNRows",			"x": "793px",			"y": "403px",			"fields": [				{					"name": "storageLevel",					"value": "DEFAULT",					"widget": "array",					"title": "Output Storage Level",					"description": "Storage Level of the Output Datasets of this Node",					"optionsArray": [						"DEFAULT",						"NONE",						"DISK_ONLY",						"DISK_ONLY_2",						"MEMORY_ONLY",						"MEMORY_ONLY_2",						"MEMORY_ONLY_SER",						"MEMORY_ONLY_SER_2",						"MEMORY_AND_DISK",						"MEMORY_AND_DISK_2",						"MEMORY_AND_DISK_SER",						"MEMORY_AND_DISK_SER_2",						"OFF_HEAP"					],					"required": false,					"display": true,					"editable": true,					"disableRefresh": false				},				{					"name": "title",					"value": "Row Values",					"widget": "textfield",					"title": "Title",					"required": false,					"display": true,					"editable": true,					"disableRefresh": false				},				{					"name": "n",					"value": "10",					"widget": "textfield",					"title": "Num Rows to Print",					"description": "number of rows to be printed",					"required": false,					"display": true,					"editable": true,					"disableRefresh": false				},				{					"name": "displayDataType",					"value": "true",					"widget": "array",					"title": "Display Data Type",					"description": "If true display rows DataType",					"optionsArray": [						"true",						"false"					],					"required": false,					"display": true,					"editable": true,					"disableRefresh": false				}			],			"engine": "all"		}	],	"edges": [		{			"source": "6",			"target": "8",			"id": 1		},		{			"source": "8",			"target": "17",			"id": 2		},		{			"source": "8",			"target": "18",			"id": 3		},		{			"source": "17",			"target": "12",			"id": 4		},		{			"source": "18",			"target": "19",			"id": 5		}	],	"dataSetDetails": [],	"engine": "scala"}