{  "name": "Neural Network with Keras",  "uuid": "79b78f8f-b721-4e06-aac3-cbbbbda5a638",  "category": "-",  "description": "-",  "parameters": "-",  "nodes": [    {      "id": "1",      "name": "ReadCSV",      "description": "It reads in CSV files and creates a DataFrame from it",      "details": "",      "examples": "",      "type": "dataset",      "nodeClass": "fire.nodes.dataset.NodeDatasetCSV",      "x": "25.1215px",      "y": "163.267px",      "fields": [        {          "name": "storageLevel",          "value": "DEFAULT",          "widget": "array",          "title": "Output Storage Level",          "description": "Storage Level of the Output Datasets of this Node",          "optionsArray": [            "DEFAULT",            "NONE",            "DISK_ONLY",            "DISK_ONLY_2",            "MEMORY_ONLY",            "MEMORY_ONLY_2",            "MEMORY_ONLY_SER",            "MEMORY_ONLY_SER_2",            "MEMORY_AND_DISK",            "MEMORY_AND_DISK_2",            "MEMORY_AND_DISK_SER",            "MEMORY_AND_DISK_SER_2",            "OFF_HEAP"          ],          "required": false,          "display": true,          "editable": true,          "disableRefresh": false        },        {          "name": "path",          "value": "data/pima-indians-diabetes.data.csv",          "widget": "textfield",          "title": "Path",          "description": "Path of the Text file/directory",          "required": true,          "display": true,          "editable": true,          "disableRefresh": false        },        {          "name": "separator",          "value": ",",          "widget": "textfield",          "title": "Separator",          "description": "CSV Separator",          "required": false,          "display": true,          "editable": true,          "disableRefresh": false        },        {          "name": "header",          "value": "false",          "widget": "array",          "title": "Header",          "description": "Does the file have a header row",          "optionsArray": [            "true",            "false"          ],          "required": false,          "display": true,          "editable": true,          "disableRefresh": false        },        {          "name": "dropMalformed",          "value": "false",          "widget": "array",          "title": "Drop Malformed",          "description": "Whether to drop Malformed records or error",          "optionsArray": [            "true",            "false"          ],          "required": false,          "display": true,          "editable": true,          "disableRefresh": false        },        {          "name": "outputColNames",          "value": "[\"_c0\",\"_c1\",\"_c2\",\"_c3\",\"_c4\",\"_c5\",\"_c6\",\"_c7\",\"_c8\"]",          "widget": "schema_col_names",          "title": "Column Names for the CSV",          "description": "New Output Columns of the SQL",          "required": false,          "display": true,          "editable": true,          "disableRefresh": false        },        {          "name": "outputColTypes",          "value": "[\"STRING\",\"STRING\",\"STRING\",\"STRING\",\"STRING\",\"STRING\",\"STRING\",\"STRING\",\"STRING\"]",          "widget": "schema_col_types",          "title": "Column Types for the CSV",          "description": "Data Type of the Output Columns",          "required": false,          "display": true,          "editable": true,          "disableRefresh": false        },        {          "name": "outputColFormats",          "value": "[\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\"]",          "widget": "schema_col_formats",          "title": "Column Formats for the CSV",          "description": "Format of the Output Columns",          "required": false,          "display": true,          "editable": true,          "disableRefresh": false        }      ],      "engine": "all"    },    {      "id": "2",      "name": "PrintNRows",      "description": "Prints the specified number of records in the DataFrame. It is useful for seeing intermediate output",      "details": "",      "examples": "",      "type": "transform",      "nodeClass": "fire.nodes.util.NodePrintFirstNRows",      "x": "441.598px",      "y": "160.754px",      "fields": [        {          "name": "storageLevel",          "value": "DEFAULT",          "widget": "array",          "title": "Output Storage Level",          "description": "Storage Level of the Output Datasets of this Node",          "optionsArray": [            "DEFAULT",            "NONE",            "DISK_ONLY",            "DISK_ONLY_2",            "MEMORY_ONLY",            "MEMORY_ONLY_2",            "MEMORY_ONLY_SER",            "MEMORY_ONLY_SER_2",            "MEMORY_AND_DISK",            "MEMORY_AND_DISK_2",            "MEMORY_AND_DISK_SER",            "MEMORY_AND_DISK_SER_2",            "OFF_HEAP"          ],          "required": false,          "display": true,          "editable": true,          "disableRefresh": false        },        {          "name": "title",          "value": "Row Values",          "widget": "textfield",          "title": "Title",          "required": false,          "display": true,          "editable": true,          "disableRefresh": false        },        {          "name": "n",          "value": "10",          "widget": "textfield",          "title": "Num Rows to Print",          "description": "number of rows to be printed",          "required": false,          "display": true,          "editable": true,          "disableRefresh": false        }      ],      "engine": "all"    },    {      "id": "3",      "name": "PySpark",      "description": "This node runs any given PySpark code. The input dataframe is passed in the variable inDF. The output dataframe is passed back by registering it as a temporary table.",      "details": "",      "examples": "",      "type": "pyspark",      "nodeClass": "fire.nodes.etl.NodePySpark",      "x": "655.328px",      "y": "157.516px",      "fields": [        {          "name": "storageLevel",          "value": "DEFAULT",          "widget": "array",          "title": "Output Storage Level",          "description": "Storage Level of the Output Datasets of this Node",          "optionsArray": [            "DEFAULT",            "NONE",            "DISK_ONLY",            "DISK_ONLY_2",            "MEMORY_ONLY",            "MEMORY_ONLY_2",            "MEMORY_ONLY_SER",            "MEMORY_ONLY_SER_2",            "MEMORY_AND_DISK",            "MEMORY_AND_DISK_2",            "MEMORY_AND_DISK_SER",            "MEMORY_AND_DISK_SER_2",            "OFF_HEAP"          ],          "required": false,          "display": true,          "editable": true,          "disableRefresh": false        },        {          "name": "code",          "value": "from pyspark.sql.types import StringType\nfrom pyspark.sql.functions import *\nfrom pyspark.sql import *\n\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nimport numpy\n\nfrom fire.workflowcontext import *\n\ndef myfn(spark: SparkSession, wc: WorkflowContext, id: int, inDF: DataFrame, cust_dict):\n  # Convert the Spark DataFrame to a Pandas DataFrame using Arrow\n  pdf = inDF.select(\"*\").toPandas()\n  \n  dataset = pdf.values\n  \n  # split into input (X) and output (Y) variables\n  X = dataset[:,0:8]\n  Y = dataset[:,8]\n\n  # create model\n  model = Sequential()\n  model.add(Dense(12, input_dim=8, activation='relu'))\n  model.add(Dense(8, activation='relu'))\n  model.add(Dense(1, activation='sigmoid'))\n\n  # Compile model\n  model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n\n  # Fit the model\n  model.fit(X, Y, epochs=150, batch_size=10)\n  \n  # evaluate the model\n  scores = model.evaluate(X, Y)\n\n  return inDF\n\n    \n",          "widget": "textarea_large",          "title": "PySpark",          "description": "PySpark code to be run. Input dataframe : \"inDF\", SparkContext : \"sc\", SQLContext : \"sqlContext\",  Output/Result dataframe should be registered as a temporary table - df.registerTempTable(\"outDF\")",          "required": false,          "display": true,          "editable": true,          "disableRefresh": false        },        {          "name": "schema",          "value": "",          "widget": "tab",          "title": "Schema",          "required": false,          "display": true,          "editable": true,          "disableRefresh": false        },        {          "name": "outputColNames",          "value": "[\"id\",\"price\",\"lotsize\",\"bedrooms\",\"bathrms\",\"stories\",\"driveway\",\"recroom\",\"fullbase\",\"gashw\",\"airco\",\"garagepl\",\"prefarea\"]",          "widget": "schema_col_names",          "title": "Column Names for the CSV",          "description": "New Output Columns of the SQL",          "required": false,          "display": true,          "editable": true,          "disableRefresh": false        },        {          "name": "outputColTypes",          "value": "[\"STRING\",\"STRING\",\"STRING\",\"STRING\",\"STRING\",\"STRING\",\"STRING\",\"STRING\",\"STRING\",\"STRING\",\"STRING\",\"STRING\",\"STRING\"]",          "widget": "schema_col_types",          "title": "Column Types for the CSV",          "description": "Data Type of the Output Columns",          "required": false,          "display": true,          "editable": true,          "disableRefresh": false        },        {          "name": "outputColFormats",          "value": "[\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\"]",          "widget": "schema_col_formats",          "title": "Column Formats for the CSV",          "description": "Format of the Output Columns",          "required": false,          "display": true,          "editable": true,          "disableRefresh": false        }      ],      "engine": "pyspark"    },    {      "id": "5",      "name": "PrintNRows",      "description": "Prints the specified number of records in the DataFrame. It is useful for seeing intermediate output",      "details": "",      "examples": "",      "type": "transform",      "nodeClass": "fire.nodes.util.NodePrintFirstNRows",      "x": "874.383px",      "y": "155.531px",      "fields": [        {          "name": "storageLevel",          "value": "DEFAULT",          "widget": "array",          "title": "Output Storage Level",          "description": "Storage Level of the Output Datasets of this Node",          "optionsArray": [            "DEFAULT",            "NONE",            "DISK_ONLY",            "DISK_ONLY_2",            "MEMORY_ONLY",            "MEMORY_ONLY_2",            "MEMORY_ONLY_SER",            "MEMORY_ONLY_SER_2",            "MEMORY_AND_DISK",            "MEMORY_AND_DISK_2",            "MEMORY_AND_DISK_SER",            "MEMORY_AND_DISK_SER_2",            "OFF_HEAP"          ],          "required": false,          "display": true,          "editable": true,          "disableRefresh": false        },        {          "name": "title",          "value": "Row Values",          "widget": "textfield",          "title": "Title",          "required": false,          "display": true,          "editable": true,          "disableRefresh": false        },        {          "name": "n",          "value": "10",          "widget": "textfield",          "title": "Num Rows to Print",          "description": "number of rows to be printed",          "required": false,          "display": true,          "editable": true,          "disableRefresh": false        }      ],      "engine": "all"    },    {      "id": "6",      "name": "CastColumnType",      "description": "This node creates a new DataFrame by casting input columns with a new data type",      "details": "",      "examples": "",      "type": "transform",      "nodeClass": "fire.nodes.etl.NodeCastColumnType",      "x": "234.637px",      "y": "163.648px",      "fields": [        {          "name": "storageLevel",          "value": "DEFAULT",          "widget": "array",          "title": "Output Storage Level",          "description": "Storage Level of the Output Datasets of this Node",          "optionsArray": [            "DEFAULT",            "NONE",            "DISK_ONLY",            "DISK_ONLY_2",            "MEMORY_ONLY",            "MEMORY_ONLY_2",            "MEMORY_ONLY_SER",            "MEMORY_ONLY_SER_2",            "MEMORY_AND_DISK",            "MEMORY_AND_DISK_2",            "MEMORY_AND_DISK_SER",            "MEMORY_AND_DISK_SER_2",            "OFF_HEAP"          ],          "required": false,          "display": true,          "editable": true,          "disableRefresh": false        },        {          "name": "inputCols",          "value": "[\"_c0\",\"_c1\",\"_c2\",\"_c3\",\"_c4\",\"_c5\",\"_c6\",\"_c7\",\"_c8\"]",          "widget": "variables",          "title": "Columns",          "description": "Columns to be cast to new data type",          "required": false,          "display": true,          "editable": true,          "disableRefresh": false        },        {          "name": "outputColType",          "value": "FLOAT",          "widget": "array",          "title": "New Data Type",          "description": "New data type(INTEGER, DOUBLE, STRING, LONG, SHORT)",          "optionsArray": [            "BOOLEAN",            "BYTE",            "DATE",            "DECIMAL",            "DOUBLE",            "FLOAT",            "INTEGER",            "LONG",            "SHORT",            "STRING",            "TIMESTAMP"          ],          "required": false,          "display": true,          "editable": true,          "disableRefresh": false        },        {          "name": "replaceExistingCols",          "value": "true",          "widget": "array",          "title": "Replace Existing Cols",          "description": "Whether to replace existing columns or create new ones",          "optionsArray": [            "true",            "false"          ],          "required": false,          "display": true,          "editable": true,          "disableRefresh": false        }      ],      "engine": "all"    }  ],  "edges": [    {      "source": "2",      "target": "3",      "id": 1    },    {      "source": "3",      "target": "5",      "id": 2    },    {      "source": "1",      "target": "6",      "id": 3    },    {      "source": "6",      "target": "2",      "id": 4    }  ],  "dataSetDetails": [],  "engine": "pyspark"}